source('C:/Users/ambar_000/Desktop/COMP594/Blum Project/R Code/gbm/R/gbm.R')
mf <- match.call(expand.dots = FALSE)
install.packages(C:\Users\ambar_000\Desktop\COMP594\Blum Project\R Code\gbm.tar.gz, repos = NULL, type="source")
install.packages("C:\Users\ambar_000\Desktop\COMP594\Blum Project\R Code\gbm.tar.gz", repos = NULL, type="source")
install.packages("C:\\Users\\ambar_000\\Desktop\\COMP594\\Blum Project\\R Code\\gbm.tar.gz", repos = NULL, type="source")
library(gbm)
angaus.5000 <- gbm.fixed(data=model.data, gbm.x = 3:14, gbm.y = 2,
learning.rate = 0.005, tree.complexity = 5, n.trees = 5000)
setwd(
"C:/brt/")
source
source(brt.functions.R)
source("brt.functions.R")
angaus.5000 <- gbm.fixed(data=model.data, gbm.x = 3:14, gbm.y = 2,
learning.rate = 0.005, tree.complexity = 5, n.trees = 5000)
model.data <- read.csv("c:/brt/model.data.csv")
angaus.5000 <- gbm.fixed(data=model.data, gbm.x = 3:14, gbm.y = 2,
learning.rate = 0.005, tree.complexity = 5, n.trees = 5000)
tree.list <- seq(100, 5000, by = 100)
pred <- predict.gbm(angaus.5000, eval.data, n.trees = tree.list,
"response")
angaus.5000
summary(angaus.5000)
library("gbm")
traindf <- read.table("C:/Users/ambar_000/Documents/GitHub/GBMWithVariableShrinkage/data/PowerPlant/TRAINING.txt", sep="\t", header=TRUE)
testdf <- read.table("C:/Users/ambar_000/Documents/GitHub/GBMWithVariableShrinkage/data/PowerPlant/TEST.txt", sep="\t", header=TRUE)
response_column <- which(colnames(traindf) == "PE")
trainy <- traindf$PE
gbm_formula <- as.formula(paste0("PE ~ ", paste(colnames(traindf[, -response_column]), collapse = " + ")))
gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = 500, bag.fraction = 1, interaction.depth = 10)
summary.gbm(gbm_model)
predictions_gbm <- predict(gbm_model, newdata = testdf[, -response_column], n.trees = 500, type = "response")
sqrt(mean((testdf$PE - predictions_gbm)^2))
predictions_train_gbm <- predict(gbm_model, newdata = traindf[, -response_column], n.trees = 500, type = "response")
sqrt(mean((traindf$PE - predictions_train_gbm)^2))
library("gbm")
traindf <- read.table("C:/Users/ambar_000/Documents/GitHub/GBMWithVariableShrinkage/data/PowerPlant/TRAINING.txt", sep="\t", header=TRUE)
testdf <- read.table("C:/Users/ambar_000/Documents/GitHub/GBMWithVariableShrinkage/data/PowerPlant/TEST.txt", sep="\t", header=TRUE)
response_column <- which(colnames(traindf) == "PE")
trainy <- traindf$PE
gbm_formula <- as.formula(paste0("PE ~ ", paste(colnames(traindf[, -response_column]), collapse = " + ")))
gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = 500, bag.fraction = 1, interaction.depth = 3)
summary.gbm(gbm_model)
predictions_gbm <- predict(gbm_model, newdata = testdf[, -response_column], n.trees = 500, type = "response")
sqrt(mean((testdf$PE - predictions_gbm)^2))
predictions_train_gbm <- predict(gbm_model, newdata = traindf[, -response_column], n.trees = 500, type = "response")
sqrt(mean((traindf$PE - predictions_train_gbm)^2))
pretty.gbm.tree(gbm1,i.tree = 1)
pretty.gbm.tree(gbm_model,i.tree = 1)
pretty.gbm.tree(gbm_model,i.tree = 0)
pretty.gbm.tree(gbm_model,i.tree = 1)
pretty.gbm.tree(gbm_model,i.tree = 2)
clear
cls
library("gbm")
traindf <- read.table("C:/Users/ambar_000/Documents/GitHub/GBMWithVariableShrinkage/data/PowerPlant/TRAINING.txt", sep="\t", header=TRUE)
testdf <- read.table("C:/Users/ambar_000/Documents/GitHub/GBMWithVariableShrinkage/data/PowerPlant/TEST.txt", sep="\t", header=TRUE)
response_column <- which(colnames(traindf) == "PE")
trainy <- traindf$PE
gbm_formula <- as.formula(paste0("PE ~ ", paste(colnames(traindf[, -response_column]), collapse = " + ")))
gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = 500, bag.fraction = 1, interaction.depth = 3)
summary.gbm(gbm_model)
predictions_gbm <- predict(gbm_model, newdata = testdf[, -response_column], n.trees = 500, type = "response")
sqrt(mean((testdf$PE - predictions_gbm)^2))
predictions_train_gbm <- predict(gbm_model, newdata = traindf[, -response_column], n.trees = 500, type = "response")
sqrt(mean((traindf$PE - predictions_train_gbm)^2))
pretty.gbm.tree(gbm_model,i.tree = 400)
pretty.gbm.tree(gbm_model,i.tree = 1)
system.time(gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = 500, bag.fraction = 1, interaction.depth = 3))
pretty.gbm.tree(gbm_model,i.tree = 2)
pretty.gbm.tree(gbm_model,i.tree = 400)
system.time(gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = 5000, bag.fraction = 1, interaction.depth = 3))
response_column <- which(colnames(traindf) == "PE")
trainy <- traindf$PE
gbm_formula <- as.formula(paste0("PE ~ ", paste(colnames(traindf[, -response_column]), collapse = " + ")))
system.time(gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = 5000, bag.fraction = 1, interaction.depth = 3))
summary.gbm(gbm_model)
predictions_gbm <- predict(gbm_model, newdata = testdf[, -response_column], n.trees = 500, type = "response")
sqrt(mean((testdf$PE - predictions_gbm)^2))
predictions_train_gbm <- predict(gbm_model, newdata = traindf[, -response_column], n.trees = 500, type = "response")
sqrt(mean((traindf$PE - predictions_train_gbm)^2))
pretty.gbm.tree(gbm_model,i.tree = 1)
pretty.gbm.tree(gbm_model,i.tree = 2)
pretty.gbm.tree(gbm_model,i.tree = 400)
predictions_gbm <- predict(gbm_model, newdata = testdf[, -response_column], n.trees = 5000, type = "response")
sqrt(mean((testdf$PE - predictions_gbm)^2))
predictions_train_gbm <- predict(gbm_model, newdata = traindf[, -response_column], n.trees = 5000, type = "response")
sqrt(mean((traindf$PE - predictions_train_gbm)^2))
system.time(gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = 50000, bag.fraction = 1, interaction.depth = 3))
system.time(gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = 50000, bag.fraction = 1, interaction.depth = 3))
summary.gbm(gbm_model)
predictions_gbm <- predict(gbm_model, newdata = testdf[, -response_column], n.trees = 50000, type = "response")
sqrt(mean((testdf$PE - predictions_gbm)^2))
predictions_train_gbm <- predict(gbm_model, newdata = traindf[, -response_column], n.trees = 50000, type = "response")
sqrt(mean((traindf$PE - predictions_train_gbm)^2))
relative.inf.gbm
relative.inf
relative.influence
relative.influence(gbm_model)
v
relative.influence2 <- function(object,
n.trees,
scale. = FALSE,
sort. = FALSE )
{
if( missing( n.trees ) ){
if ( object$train.fraction < 1 ){
n.trees <- gbm.perf( object, method="test", plot.it=FALSE )
}
else if ( !is.null( object$cv.error ) ){
n.trees <- gbm.perf( object, method="cv", plot.it = FALSE )
}
else{
# If dist=multinomial, object$n.trees = n.trees * num.classes
# so use the following instead.
n.trees <- length( object$train.error )
}
cat( paste( "n.trees not given. Using", n.trees, "trees.\n" ) )
if (object$distribution == "multinomial"){
n.trees <- n.trees * object$num.classes
}
}
get.rel.inf <- function(obj)
{
0 #lapply(split(obj[[6]],obj[[1]]),sum) # 6 - Improvement, 1 - var name
}
temp <- unlist(lapply(object$trees[1:n.trees],get.rel.inf))
rel.inf.compact <- unlist(lapply(split(temp,names(temp)),sum))
rel.inf.compact <- rel.inf.compact[names(rel.inf.compact)!="-1"]
# rel.inf.compact excludes those variable that never entered the model
# insert 0's for the excluded variables
rel.inf <- rep(0,length(object$var.names))
i <- as.numeric(names(rel.inf.compact))+1
rel.inf[i] <- rel.inf.compact
names(rel.inf) <- object$var.names
if (scale.){
rel.inf <- rel.inf / max(rel.inf)
}
if (sort.){
rel.inf <- rev(sort(rel.inf))
}
return(rel.inf=rel.inf)
}
summary.gbm
summary.gbm()
summary.gbm(gbm_model)
gbm_formula <- as.formula(paste0("PE ~ ", paste(colnames(traindf[, -response_column]), collapse = " + ")))
system.time(gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = 500, bag.fraction = 1, interaction.depth = 3))
summary.gbm(gbm_model)
predictions_gbm <- predict(gbm_model, newdata = testdf[, -response_column], n.trees = 500, type = "response")
sqrt(mean((testdf$PE - predictions_gbm)^2))
predictions_train_gbm <- predict(gbm_model, newdata = traindf[, -response_column], n.trees = 500, type = "response")
sqrt(mean((traindf$PE - predictions_train_gbm)^2))
relative.influence(gbm_model)
relative.influence(gbm_model, n.tree=500)
summary.gbm <- function(object,
cBars=length(object$var.names),
n.trees=object$n.trees,
plotit=TRUE,
order=TRUE,
method=relative.influence2,
normalize=TRUE,
...)
{
if(n.trees < 1)
{
stop("n.trees must be greater than 0.")
}
if(n.trees > object$n.trees)
{
warning("Exceeded total number of GBM terms. Results use n.trees=",object$n.trees," terms.\n")
n.trees <- object$n.trees
}
rel.inf <- method(object,n.trees)
rel.inf[rel.inf<0] <- 0
if(order)
{
i <- order(-rel.inf)
}
else
{
i <- 1:length(rel.inf)
}
if(cBars==0) cBars <- min(10,length(object$var.names))
if(cBars>length(object$var.names)) cBars <- length(object$var.names)
if(normalize) rel.inf <- 100*rel.inf/sum(rel.inf)
if(plotit)
{
barplot(rel.inf[i[cBars:1]],
horiz=TRUE,
col=rainbow(cBars,start=3/6,end=4/6),
names=object$var.names[i[cBars:1]],
xlab="Relative influence",...)
}
return(data.frame(var=object$var.names[i],
rel.inf=rel.inf[i]))
}
summary.gbm(gbm_model)
gbm.show(gbm_model)
show.gbm(gbm_model)
print.gbm(gbm_model)
relative.influence2 <- function(object,
n.trees,
scale. = FALSE,
sort. = FALSE )
{
if( missing( n.trees ) ){
if ( object$train.fraction < 1 ){
n.trees <- gbm.perf( object, method="test", plot.it=FALSE )
}
else if ( !is.null( object$cv.error ) ){
n.trees <- gbm.perf( object, method="cv", plot.it = FALSE )
}
else{
# If dist=multinomial, object$n.trees = n.trees * num.classes
# so use the following instead.
n.trees <- length( object$train.error )
}
cat( paste( "n.trees not given. Using", n.trees, "trees.\n" ) )
if (object$distribution == "multinomial"){
n.trees <- n.trees * object$num.classes
}
}
get.rel.inf <- function(obj)
{
print(obj[[6]])
lapply(split(obj[[6]],obj[[1]]),sum) # 6 - Improvement, 1 - var name
}
temp <- unlist(lapply(object$trees[1:n.trees],get.rel.inf))
rel.inf.compact <- unlist(lapply(split(temp,names(temp)),sum))
rel.inf.compact <- rel.inf.compact[names(rel.inf.compact)!="-1"]
# rel.inf.compact excludes those variable that never entered the model
# insert 0's for the excluded variables
rel.inf <- rep(0,length(object$var.names))
i <- as.numeric(names(rel.inf.compact))+1
rel.inf[i] <- rel.inf.compact
names(rel.inf) <- object$var.names
if (scale.){
rel.inf <- rel.inf / max(rel.inf)
}
if (sort.){
rel.inf <- rev(sort(rel.inf))
}
return(rel.inf=rel.inf)
}
summary.gbm(gbm_model)
gbm_formula <- as.formula(paste0("PE ~ ", paste(colnames(traindf[, -response_column]), collapse = " + ")))
system.time(gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = 50000, bag.fraction = 1, interaction.depth = 3))
summary.gbm(gbm_model)
predictions_gbm <- predict(gbm_model, newdata = testdf[, -response_column], n.trees = 50000, type = "response")
sqrt(mean((testdf$PE - predictions_gbm)^2))
predictions_train_gbm <- predict(gbm_model, newdata = traindf[, -response_column], n.trees = 50000, type = "response")
sqrt(mean((traindf$PE - predictions_train_gbm)^2))
pretty.gbm.tree(gbm_model,i.tree = 1)
pretty.gbm.tree(gbm_model,i.tree = 2)
pretty.gbm.tree(gbm_model,i.tree = 50000)
summary.gbm <- function(object,
cBars=length(object$var.names),
n.trees=object$n.trees,
plotit=TRUE,
order=TRUE,
method=relative.influence,
normalize=TRUE,
...)
{
if(n.trees < 1)
{
stop("n.trees must be greater than 0.")
}
if(n.trees > object$n.trees)
{
warning("Exceeded total number of GBM terms. Results use n.trees=",object$n.trees," terms.\n")
n.trees <- object$n.trees
}
rel.inf <- method(object,n.trees)
rel.inf[rel.inf<0] <- 0
if(order)
{
i <- order(-rel.inf)
}
else
{
i <- 1:length(rel.inf)
}
if(cBars==0) cBars <- min(10,length(object$var.names))
if(cBars>length(object$var.names)) cBars <- length(object$var.names)
if(normalize) rel.inf <- 100*rel.inf/sum(rel.inf)
if(plotit)
{
barplot(rel.inf[i[cBars:1]],
horiz=TRUE,
col=rainbow(cBars,start=3/6,end=4/6),
names=object$var.names[i[cBars:1]],
xlab="Relative influence",...)
}
return(data.frame(var=object$var.names[i],
rel.inf=rel.inf[i]))
}
system.time(gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = 50000, bag.fraction = 1, interaction.depth = 3))
summary.gbm(gbm_model)
predictions_gbm <- predict(gbm_model, newdata = testdf[, -response_column], n.trees = 50000, type = "response")
sqrt(mean((testdf$PE - predictions_gbm)^2))
predictions_train_gbm <- predict(gbm_model, newdata = traindf[, -response_column], n.trees = 50000, type = "response")
sqrt(mean((traindf$PE - predictions_train_gbm)^2))
pretty.gbm.tree(gbm_model,i.tree = 1)
pretty.gbm.tree(gbm_model,i.tree = 150)
pretty.gbm.tree(gbm_model,i.tree = 30000)
pretty.gbm.tree(gbm_model,i.tree = 50000)
system.time(gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = 500, bag.fraction = 1, interaction.depth = 3))
summary.gbm(gbm_model)
predictions_gbm <- predict(gbm_model, newdata = testdf[, -response_column], n.trees = 500, type = "response")
sqrt(mean((testdf$PE - predictions_gbm)^2))
predictions_train_gbm <- predict(gbm_model, newdata = traindf[, -response_column], n.trees = 500, type = "response")
sqrt(mean((traindf$PE - predictions_train_gbm)^2))
pretty.gbm.tree(gbm_model,i.tree = 1)
pretty.gbm.tree(gbm_model,i.tree = 150)
pretty.gbm.tree(gbm_model,i.tree = 500)
system.time(gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = 150, bag.fraction = 1, interaction.depth = 3))
summary.gbm(gbm_model)
predictions_gbm <- predict(gbm_model, newdata = testdf[, -response_column], n.trees = 150, type = "response")
sqrt(mean((testdf$PE - predictions_gbm)^2))
predictions_train_gbm <- predict(gbm_model, newdata = traindf[, -response_column], n.trees = 150, type = "response")
sqrt(mean((traindf$PE - predictions_train_gbm)^2))
pretty.gbm.tree(gbm_model,i.tree = 1)
pretty.gbm.tree(gbm_model,i.tree = 150)
system.time(gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = 7, bag.fraction = 1, interaction.depth = 3))
summary.gbm(gbm_model)
predictions_gbm <- predict(gbm_model, newdata = testdf[, -response_column], n.trees = 7, type = "response")
sqrt(mean((testdf$PE - predictions_gbm)^2))
predictions_train_gbm <- predict(gbm_model, newdata = traindf[, -response_column], n.trees = 7, type = "response")
sqrt(mean((traindf$PE - predictions_train_gbm)^2))
pretty.gbm.tree(gbm_model,i.tree = 1)
pretty.gbm.tree(gbm_model,i.tree = 150)
pretty.gbm.tree(gbm_model,i.tree = 2)
pretty.gbm.tree(gbm_model,i.tree = 3)
pretty.gbm.tree(gbm_model,i.tree = 10)
pretty.gbm.tree(gbm_model,i.tree = 10)
system.time(gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = 150, bag.fraction = 1, interaction.depth = 3))
summary.gbm(gbm_model)
predictions_gbm <- predict(gbm_model, newdata = testdf[, -response_column], n.trees = 150, type = "response")
sqrt(mean((testdf$PE - predictions_gbm)^2))
predictions_train_gbm <- predict(gbm_model, newdata = traindf[, -response_column], n.trees = 150, type = "response")
sqrt(mean((traindf$PE - predictions_train_gbm)^2))
pretty.gbm.tree(gbm_model,i.tree = 1)
pretty.gbm.tree(gbm_model,i.tree = 150)
149
gbm_formula <- as.formula(paste0("PE ~ ", paste(colnames(traindf[, -response_column]), collapse = " + ")))
system.time(gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = 10000, bag.fraction = 1, interaction.depth = 3))
summary.gbm(gbm_model)
predictions_gbm <- predict(gbm_model, newdata = testdf[, -response_column], n.trees = 10000, type = "response")
sqrt(mean((testdf$PE - predictions_gbm)^2))
predictions_train_gbm <- predict(gbm_model, newdata = traindf[, -response_column], n.trees = 10000, type = "response")
sqrt(mean((traindf$PE - predictions_train_gbm)^2))
pretty.gbm.tree(gbm_model,i.tree = 1)
pretty.gbm.tree(gbm_model,i.tree = 150)
pretty.gbm.tree(gbm_model,i.tree = 500)
traindf <- read.table("C:/Users/ambar_000/Documents/GitHub/GBMWithVariableShrinkage/data/ServoMotor/TRAINING_2.txt", sep="\t", header=TRUE)
testdf <- read.table("C:/Users/ambar_000/Documents/GitHub/GBMWithVariableShrinkage/data/ServoMotor/TEST_2.txt", sep="\t", header=TRUE)
response_column <- which(colnames(traindf) == "PE")
trainy <- traindf$PE
gbm_formula <- as.formula(paste0("PE ~ ", paste(colnames(traindf[, -response_column]), collapse = " + ")))
system.time(gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = 10000, bag.fraction = 1, interaction.depth = 3))
summary.gbm(gbm_model)
predictions_gbm <- predict(gbm_model, newdata = testdf[, -response_column], n.trees = 10000, type = "response")
sqrt(mean((testdf$PE - predictions_gbm)^2))
predictions_train_gbm <- predict(gbm_model, newdata = traindf[, -response_column], n.trees = 10000, type = "response")
sqrt(mean((traindf$PE - predictions_train_gbm)^2))
pretty.gbm.tree(gbm_model,i.tree = 1)
pretty.gbm.tree(gbm_model,i.tree = 150)
pretty.gbm.tree(gbm_model,i.tree = 500)
traindf <- read.table("C:/Users/ambar_000/Documents/GitHub/GBMWithVariableShrinkage/data/ServoMotor/TRAINING_2.txt", sep="\t", header=TRUE)
testdf <- read.table("C:/Users/ambar_000/Documents/GitHub/GBMWithVariableShrinkage/data/ServoMotor/TEST_2.txt", sep="\t", header=TRUE)
response_column <- which(colnames(traindf) == "class")
trainy <- traindf$class
gbm_formula <- as.formula(paste0("class ~ ", paste(colnames(traindf[, -response_column]), collapse = " + ")))
system.time(gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = 10000, bag.fraction = 1, interaction.depth = 3))
summary.gbm(gbm_model)
predictions_gbm <- predict(gbm_model, newdata = testdf[, -response_column], n.trees = 10000, type = "response")
sqrt(mean((testdf$class - predictions_gbm)^2))
predictions_train_gbm <- predict(gbm_model, newdata = traindf[, -response_column], n.trees = 10000, type = "response")
sqrt(mean((traindf$class - predictions_train_gbm)^2))
pretty.gbm.tree(gbm_model,i.tree = 1)
pretty.gbm.tree(gbm_model,i.tree = 150)
pretty.gbm.tree(gbm_model,i.tree = 500)
traindf
summary(traindf)
traindf <- read.table("C:/Users/ambar_000/Documents/GitHub/GBMWithVariableShrinkage/data/ServoMotor/TRAINING_2.txt", sep="\t", header=TRUE)
testdf <- read.table("C:/Users/ambar_000/Documents/GitHub/GBMWithVariableShrinkage/data/ServoMotor/TEST_2.txt", sep="\t", header=TRUE)
response_column <- which(colnames(traindf) == "class")
trainy <- traindf$class
gbm_formula <- as.formula(paste0("class ~ ", paste(colnames(traindf[, -response_column]), collapse = " + ")))
system.time(gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = 10000, bag.fraction = 1, interaction.depth = 3))
summary.gbm(gbm_model)
predictions_gbm <- predict(gbm_model, newdata = testdf[, -response_column], n.trees = 10000, type = "response")
sqrt(mean((testdf$class - predictions_gbm)^2))
predictions_train_gbm <- predict(gbm_model, newdata = traindf[, -response_column], n.trees = 10000, type = "response")
sqrt(mean((traindf$class - predictions_train_gbm)^2))
pretty.gbm.tree(gbm_model,i.tree = 1)
pretty.gbm.tree(gbm_model,i.tree = 150)
pretty.gbm.tree(gbm_model,i.tree = 500)
is.factor(traindf[0:])
is.factor(traindf[0])
traindf[0]
head traindf[0]
head(traindf[0])
head(traindf[0:])
head(traindf[:0])
head(traindf)
class(traindf)
is.factor(traindf$motor)
as.factor(traindf$motor)
traindf$motor
traindf$Motor
as.factor(traindf$Motor)
is.factor(traindf$Motor)
is.factor(traindf$Motor)
is.ordered(traindf$Motor)
as.factor(traindf$)
traindf <- read.table("C:/Users/ambar_000/Documents/GitHub/GBMWithVariableShrinkage/data/ServoMotor/TRAINING_2.txt", sep="\t", header=TRUE)
testdf <- read.table("C:/Users/ambar_000/Documents/GitHub/GBMWithVariableShrinkage/data/ServoMotor/TEST_2.txt", sep="\t", header=TRUE)
response_column <- which(colnames(traindf) == "class")
trainy <- traindf$class
traindf$pgain = as.factor(traindf$pgain)
traindf$vgain = as.factor(traindf$vgain)
testdf$pgain = as.factor(testdf$pgain)
testdf$vgain = as.factor(testdf$vgain)
gbm_formula <- as.formula(paste0("class ~ ", paste(colnames(traindf[, -response_column]), collapse = " + ")))
system.time(gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = 10000, bag.fraction = 1, interaction.depth = 3))
summary.gbm(gbm_model)
predictions_gbm <- predict(gbm_model, newdata = testdf[, -response_column], n.trees = 10000, type = "response")
sqrt(mean((testdf$class - predictions_gbm)^2))
predictions_train_gbm <- predict(gbm_model, newdata = traindf[, -response_column], n.trees = 10000, type = "response")
sqrt(mean((traindf$class - predictions_train_gbm)^2))
pretty.gbm.tree(gbm_model,i.tree = 1)
pretty.gbm.tree(gbm_model,i.tree = 150)
pretty.gbm.tree(gbm_model,i.tree = 500)
traindf <- read.table("C:/Users/ambar_000/Documents/GitHub/GBMWithVariableShrinkage/data/PowerPlant/TRAINING.txt", sep="\t", header=TRUE)
testdf <- read.table("C:/Users/ambar_000/Documents/GitHub/GBMWithVariableShrinkage/data/PowerPlant/TEST.txt", sep="\t", header=TRUE)
response_column <- which(colnames(traindf) == "PE")
trainy <- traindf$PE
gbm_formula <- as.formula(paste0("PE ~ ", paste(colnames(traindf[, -response_column]), collapse = " + ")))
system.time(gbm_model <- gbm(gbm_formula, traindf, distribution = "gaussian", n.trees = 10000, bag.fraction = 1, interaction.depth = 3))
summary.gbm(gbm_model)
predictions_gbm <- predict(gbm_model, newdata = testdf[, -response_column], n.trees = 10000, type = "response")
sqrt(mean((testdf$PE - predictions_gbm)^2))
predictions_train_gbm <- predict(gbm_model, newdata = traindf[, -response_column], n.trees = 10000, type = "response")
sqrt(mean((traindf$PE - predictions_train_gbm)^2))
pretty.gbm.tree(gbm_model,i.tree = 1)
pretty.gbm.tree(gbm_model,i.tree = 150)
pretty.gbm.tree(gbm_model,i.tree = 500)
gbm_formula <- as.formula(paste0("PE ~ ", paste(colnames(traindf[, -response_column]), collapse = " + ")))
gbm_formula <- as.formula(paste0("PE ~ ", paste(colnames(traindf[, -response_column]), collapse = " + ")))
