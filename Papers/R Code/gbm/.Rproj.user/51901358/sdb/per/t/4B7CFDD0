{
    "contents" : "gbm.fit <- function(x,y,\n                    offset = NULL,\n                    misc = NULL,\n                    distribution = \"bernoulli\",\n                    w = NULL,\n                    var.monotone = NULL,\n                    n.trees = 100,\n                    interaction.depth = 1,\n                    n.minobsinnode = 10,\n                    shrinkage = 0.001,\n                    bag.fraction = 0.5,\n                    nTrain = NULL,\n                    train.fraction = NULL,\n                    keep.data = TRUE,\n                    verbose = TRUE,\n                    var.names = NULL,\n                    response.name = \"y\",\n                    group = NULL)\n{\n\n  print(\"Beginning of gbm.fit\")\n  print(str(x))\n  readline()\n  \n   if(is.character(distribution)) { distribution <- list(name=distribution) }\n\n   cRows <- nrow(x)\n   cCols <- ncol(x)\n\n   if(nrow(x) != ifelse(class(y)==\"Surv\", nrow(y), length(y))) {\n      stop(\"The number of rows in x does not equal the length of y.\")\n   }\n\n   # the preferred way to specify the number of training instances is via parameter 'nTrain'.\n   # parameter 'train.fraction' is only maintained for backward compatibility.\n\n   if(!is.null(nTrain) && !is.null(train.fraction)) {\n      stop(\"Parameters 'nTrain' and 'train.fraction' cannot both be specified\")\n   }\n   else if(!is.null(train.fraction)) {\n      warning(\"Parameter 'train.fraction' of gbm.fit is deprecated, please specify 'nTrain' instead\")\n      nTrain <- floor(train.fraction*cRows)\n   }\n   else if(is.null(nTrain)) {\n     # both undefined, use all training data\n     nTrain <- cRows\n   }\n\n   if (is.null(train.fraction)){\n      train.fraction <- nTrain / cRows\n   }\n\n   if(is.null(var.names)) {\n       var.names <- getVarNames(x)\n   }\n\n#   if(is.null(response.name)) { response.name <- \"y\" }\n\n   # check dataset size\n   if(nTrain * bag.fraction <= 2*n.minobsinnode+1) {\n      stop(\"The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode\")\n   }\n\n   if (distribution$name != \"pairwise\") {\n      w <- w*length(w)/sum(w) # normalize to N\n   }\n\n   # Do sanity checks\n   ch <- checkMissing(x, y)\n   interaction.depth <- checkID(interaction.depth)\n   w <- checkWeights(w, length(y))\n   offset <- checkOffset(offset, y)\n\n   Misc <- NA\n\n   # setup variable types\n   var.type <- rep(0,cCols)\n   var.levels <- vector(\"list\",cCols)\n   for(i in 1:length(var.type))\n   {\n      if(all(is.na(x[,i])))\n      {\n         stop(\"variable \",i,\": \",var.names[i],\" has only missing values.\")\n      }\n      if(is.ordered(x[,i]))\n      {\n         var.levels[[i]] <- levels(x[,i])\n         x[,i] <- as.numeric(x[,i])-1\n         var.type[i] <- 0\n      }\n      else if(is.factor(x[,i]))\n      {\n         if(length(levels(x[,i]))>1024)\n            stop(\"gbm does not currently handle categorical variables with more than 1024 levels. Variable \",i,\": \",var.names[i],\" has \",length(levels(x[,i])),\" levels.\")\n         var.levels[[i]] <- levels(x[,i])\n         x[,i] <- as.numeric(x[,i])-1\n         var.type[i] <- max(x[,i],na.rm=TRUE)+1\n      }\n      else if(is.numeric(x[,i]))\n      {\n         var.levels[[i]] <- quantile(x[,i],prob=(0:10)/10,na.rm=TRUE)\n      }\n      else\n      {\n         stop(\"variable \",i,\": \",var.names[i],\" is not of type numeric, ordered, or factor.\")\n      }\n\n      # check for some variation in each variable\n      if(length(unique(var.levels[[i]])) == 1)\n      {\n         warning(\"variable \",i,\": \",var.names[i],\" has no variation.\")\n      }\n   }\n\n   nClass <- 1\n\n   if(!(\"name\" %in% names(distribution))) {\n      stop(\"The distribution is missing a 'name' component, for example list(name=\\\"gaussian\\\")\")\n   }\n   supported.distributions <-\n   c(\"bernoulli\",\"gaussian\",\"poisson\",\"adaboost\",\"laplace\",\"coxph\",\"quantile\",\n     \"tdist\", \"multinomial\", \"huberized\", \"pairwise\")\n\n   distribution.call.name <- distribution$name\n\n   # check potential problems with the distributions\n   if(!is.element(distribution$name,supported.distributions))\n   {\n      stop(\"Distribution \",distribution$name,\" is not supported\")\n   }\n   if((distribution$name == \"bernoulli\") && !all(is.element(y,0:1)))\n   {\n      stop(\"Bernoulli requires the response to be in {0,1}\")\n   }\n   if((distribution$name == \"huberized\") && !all(is.element(y,0:1)))\n   {\n      stop(\"Huberized square hinged loss requires the response to be in {0,1}\")\n   }\n   if((distribution$name == \"poisson\") && any(y<0))\n   {\n      stop(\"Poisson requires the response to be positive\")\n   }\n   if((distribution$name == \"poisson\") && any(y != trunc(y)))\n   {\n      stop(\"Poisson requires the response to be a positive integer\")\n   }\n   if((distribution$name == \"adaboost\") && !all(is.element(y,0:1)))\n   {\n      stop(\"This version of AdaBoost requires the response to be in {0,1}\")\n   }\n   if(distribution$name == \"quantile\")\n   {\n      if(length(unique(w)) > 1)\n      {\n         stop(\"This version of gbm for the quantile regression lacks a weighted quantile. For now the weights must be constant.\")\n      }\n      if(is.null(distribution$alpha))\n      {\n         stop(\"For quantile regression, the distribution parameter must be a list with a parameter 'alpha' indicating the quantile, for example list(name=\\\"quantile\\\",alpha=0.95).\")\n      } else\n      if((distribution$alpha<0) || (distribution$alpha>1))\n      {\n         stop(\"alpha must be between 0 and 1.\")\n      }\n      Misc <- c(alpha=distribution$alpha)\n   }\n   if(distribution$name == \"coxph\")\n   {\n      if(class(y)!=\"Surv\")\n      {\n         stop(\"Outcome must be a survival object Surv(time,failure)\")\n      }\n      if(attr(y,\"type\")!=\"right\")\n      {\n         stop(\"gbm() currently only handles right censored observations\")\n      }\n      Misc <- y[,2]\n      y <- y[,1]\n\n      # reverse sort the failure times to compute risk sets on the fly\n      i.train <- order(-y[1:nTrain])\n      n.test <- cRows - nTrain\n      if(n.test > 0)\n      {\n         i.test <- order(-y[(nTrain+1):cRows]) + nTrain\n      }\n      else\n      {\n         i.test <- NULL\n      }\n      i.timeorder <- c(i.train,i.test)\n\n      y <- y[i.timeorder]\n      Misc <- Misc[i.timeorder]\n      x <- x[i.timeorder,,drop=FALSE]\n      w <- w[i.timeorder]\n      if(!is.na(offset)) offset <- offset[i.timeorder]\n   }\n   if(distribution$name == \"tdist\")\n   {\n      if (is.null(distribution$df) || !is.numeric(distribution$df)){\n         Misc <- 4\n      }\n      else {\n         Misc <- distribution$df[1]\n      }\n   }\n   if (distribution$name == \"multinomial\")\n   {\n      ## Ensure that the training set contains all classes\n      classes <- attr(factor(y), \"levels\")\n      nClass <- length(classes)\n\n      if (nClass > nTrain){\n         stop(paste(\"Number of classes (\", nClass,\n                    \") must be less than the size of the training set (\", nTrain, \")\",\n                    sep = \"\"))\n      }\n\n      #    f <- function(a,x){\n      #       min((1:length(x))[x==a])\n      #    }\n\n      new.idx <- as.vector(sapply(classes, function(a,x){ min((1:length(x))[x==a]) }, y))\n\n      all.idx <- 1:length(y)\n      new.idx <- c(new.idx, all.idx[!(all.idx %in% new.idx)])\n\n      y <- y[new.idx]\n      x <- x[new.idx, ]\n      w <- w[new.idx]\n      if (!is.null(offset)){\n         offset <- offset[new.idx]\n      }\n\n      ## Get the factors\n      y <- as.numeric(as.vector(outer(y, classes, \"==\")))\n\n      ## Fill out the weight and offset\n      w <- rep(w, nClass)\n      if (!is.null(offset)){\n         offset <- rep(offset, nClass)\n      }\n   } # close if (dist... == \"multinomial\"\n\n   if(distribution$name == \"pairwise\")\n   {\n      distribution.metric <- distribution[[\"metric\"]]\n      if (!is.null(distribution.metric))\n      {\n         distribution.metric <- tolower(distribution.metric)\n         supported.metrics <- c(\"conc\", \"ndcg\", \"map\", \"mrr\")\n         if (!is.element(distribution.metric, supported.metrics))\n         {\n            stop(\"Metric '\", distribution.metric, \"' is not supported, use either 'conc', 'ndcg', 'map', or 'mrr'\")\n         }\n         metric <- distribution.metric\n      }\n      else\n      {\n         warning(\"No metric specified, using 'ndcg'\")\n         metric <- \"ndcg\" # default\n         distribution[[\"metric\"]] <- metric\n      }\n\n      if (any(y<0))\n      {\n         stop(\"targets for 'pairwise' should be non-negative\")\n      }\n\n      if (is.element(metric, c(\"mrr\", \"map\")) && (!all(is.element(y, 0:1))))\n      {\n         stop(\"Metrics 'map' and 'mrr' require the response to be in {0,1}\")\n      }\n\n      # Cut-off rank for metrics\n      # Default of 0 means no cutoff\n\n      max.rank <- 0\n      if (!is.null(distribution[[\"max.rank\"]]) && distribution[[\"max.rank\"]] > 0)\n      {\n         if (is.element(metric, c(\"ndcg\", \"mrr\")))\n         {\n            max.rank <- distribution[[\"max.rank\"]]\n         }\n         else\n         {\n            stop(\"Parameter 'max.rank' cannot be specified for metric '\", distribution.metric, \"', only supported for 'ndcg' and 'mrr'\")\n         }\n      }\n\n      # We pass the cut-off rank to the C function as the last element in the Misc vector\n      Misc <- c(group, max.rank)\n\n      distribution.call.name <- sprintf(\"pairwise_%s\", metric)\n   } # close if (dist... == \"pairwise\"\n\n   # create index upfront... subtract one for 0 based order\n   x.order <- apply(x[1:nTrain,,drop=FALSE],2,order,na.last=FALSE)-1\n\n   x <- as.vector(data.matrix(x))\n   predF <- rep(0,length(y))\n   train.error <- rep(0,n.trees)\n   valid.error <- rep(0,n.trees)\n   oobag.improve <- rep(0,n.trees)\n\n   if(is.null(var.monotone)) var.monotone <- rep(0,cCols)\n   else if(length(var.monotone)!=cCols)\n   {\n      stop(\"Length of var.monotone != number of predictors\")\n   }\n   else if(!all(is.element(var.monotone,-1:1)))\n   {\n      stop(\"var.monotone must be -1, 0, or 1\")\n   }\n   fError <- FALSE\n  print(\"Just before .Call gbm.fit\")\n  print(str(x))\n  readline()\n   gbm.obj <- .Call(\"gbm\",\n                    Y=as.double(y),\n                    Offset=as.double(offset),\n                    X=as.double(x),\n                    X.order=as.integer(x.order),\n                    weights=as.double(w),\n                    Misc=as.double(Misc),\n                    cRows=as.integer(cRows),\n                    cCols=as.integer(cCols),\n                    var.type=as.integer(var.type),\n                    var.monotone=as.integer(var.monotone),\n                    distribution=as.character(distribution.call.name),\n                    n.trees=as.integer(n.trees),\n                    interaction.depth=as.integer(interaction.depth),\n                    n.minobsinnode=as.integer(n.minobsinnode),\n                    n.classes = as.integer(nClass),\n                    shrinkage=as.double(shrinkage),\n                    bag.fraction=as.double(bag.fraction),\n                    nTrain=as.integer(nTrain),\n                    fit.old=as.double(NA),\n                    n.cat.splits.old=as.integer(0),\n                    n.trees.old=as.integer(0),\n                    verbose=as.integer(verbose),\n                    PACKAGE = \"gbm\")\n\n   names(gbm.obj) <- c(\"initF\",\"fit\",\"train.error\",\"valid.error\",\n                       \"oobag.improve\",\"trees\",\"c.splits\")\n\n   gbm.obj$bag.fraction <- bag.fraction\n   gbm.obj$distribution <- distribution\n   gbm.obj$interaction.depth <- interaction.depth\n   gbm.obj$n.minobsinnode <- n.minobsinnode\n   gbm.obj$num.classes <- nClass\n   gbm.obj$n.trees <- length(gbm.obj$trees) / nClass\n   gbm.obj$nTrain <- nTrain\n   gbm.obj$train.fraction <- train.fraction\n   gbm.obj$response.name <- response.name\n   gbm.obj$shrinkage <- shrinkage\n   gbm.obj$var.levels <- var.levels\n   gbm.obj$var.monotone <- var.monotone\n   gbm.obj$var.names <- var.names\n   gbm.obj$var.type <- var.type\n   gbm.obj$verbose <- verbose\n   gbm.obj$Terms <- NULL\n\n   if(distribution$name == \"coxph\")\n   {\n      gbm.obj$fit[i.timeorder] <- gbm.obj$fit\n   }\n   ## If K-Classification is used then split the fit and tree components\n   if (distribution$name == \"multinomial\"){\n      gbm.obj$fit <- matrix(gbm.obj$fit, ncol = nClass)\n      dimnames(gbm.obj$fit)[[2]] <- classes\n      gbm.obj$classes <- classes\n\n      ## Also get the class estimators\n      exp.f <- exp(gbm.obj$fit)\n      denom <- matrix(rep(rowSums(exp.f), nClass), ncol = nClass)\n      gbm.obj$estimator <- exp.f/denom\n   }\n\n   if(keep.data)\n   {\n      if(distribution$name == \"coxph\")\n      {\n         # put the observations back in order\n         gbm.obj$data <- list(y=y,x=x,x.order=x.order,offset=offset,Misc=Misc,w=w,\n                              i.timeorder=i.timeorder)\n      }\n      else if ( distribution$name == \"multinomial\" ){\n         # Restore original order of the data\n         new.idx <- order( new.idx )\n         gbm.obj$data <- list( y=as.vector(matrix(y, ncol=length(classes),byrow=FALSE)[new.idx,]),\n                              x=as.vector(matrix(x, ncol=length(var.names), byrow=FALSE)[new.idx,]),\n                              x.order=x.order,\n                              offset=offset[new.idx],\n                              Misc=Misc, w=w[new.idx] )\n      }\n      else\n      {\n         gbm.obj$data <- list(y=y,x=x,x.order=x.order,offset=offset,Misc=Misc,w=w)\n      }\n   }\n   else\n   {\n      gbm.obj$data <- NULL\n   }\n\n   class(gbm.obj) <- \"gbm\"\n   return(gbm.obj)\n}\n",
    "created" : 1442532661140.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1028024529",
    "id" : "4B7CFDD0",
    "lastKnownWriteTime" : 1442553352,
    "path" : "C:/Users/ambar_000/Desktop/COMP594/Blum Project/R Code/gbm/R/gbm.fit.R",
    "project_path" : "R/gbm.fit.R",
    "properties" : {
    },
    "source_on_save" : true,
    "type" : "r_source"
}