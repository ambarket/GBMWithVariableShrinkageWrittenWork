\documentclass[9pt, conference]{IEEEtran}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)

% *** CITATION PACKAGES ***
%
\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/tex-archive/info/epslatex/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/




% *** FLOAT PACKAGES ***
%
\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Do not use the stfloats baselinefloat ability as IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/dblfloatfix/




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/url/
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{placeins}
\usepackage{fancyvrb}
\usepackage[bf]{caption2}
\usepackage{comment}
\usepackage{hhline}
\usepackage{algorithm2e}

\usepackage{filecontents,lipsum}
\usepackage{todonotes}
\usepackage{xcolor}% you could also use the color package
\usepackage{colortbl}
\usepackage{lipsum}
\usepackage{verbatim}
\usepackage{import}
\makeatletter
\newcommand{\removelatexerror}{\let\@latex@error\@gobble}
\makeatother

\extrafloats{1000}

%\makeatletter
%\@dblfptop 0pt
%\makeatother

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Gradient Boosting Regression Trees with Variable Shrinkage}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Austin Barket}
\IEEEauthorblockA{Department of Computer Science\\
The Pennsylvania State University at Harrisburg\\
Middletown, PA 17057\\
Email: amb6470@psu.edu}
\and
\IEEEauthorblockN{Jeremy Blum}
\IEEEauthorblockA{Department of Computer Science\\
The Pennsylvania State University at Harrisburg\\
Middletown, PA 17057\\
Email: jjb24@psu.edu}
}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
In this research we explore the effect of variable shrinkage (learning rate) on gradient boosting machines that utilize regression trees as the base learners. Conventionally, research and implementations of gradient boosting machines have used only constant shrinkage that is applied uniformly to the predictions of all base learners.
\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}

Boosting is a machine learning technique that combines learning algorithms that barely beat random guessing, known as a weak or base learners, into a single model with significantly improved accuracy or lower error rates over any of its constituent parts \cite{StrengthOfWeak}  \cite{BoostingSurvey}. 

The gradient boosting machine (GBM), originally introduced by Friedman in 1999 is a general boosting framework that leverages the steepest descent numerical optimization method to iteratively train base learners to address the errors made by those before them \cite{2001Friedman}.

Production implementations of gradient boosting machines such as the gbm package in R have found remarkable traction among researchers in a wide variety of fields including robotics and ecology \cite{GBMTut} \cite{ecological}. Interestingly these packages generally implement Friedman's Gradient Boost algorithm as it was originally defined \cite{2012ridgeway}, leaving some definite opportunity for research into algorithmic tweaks to improve performance. 

A particular component of the algorithm that has not been explored to date is the learning rate, commonly referred to as shrinkage in the case of GBMs. Shrinkage is traditionally implemented as a constant parameter to the model. After each iteration the new base learner's predictions are scaled by the shrinkage as a form of regularization. 

This proposal outlines a new way to think about shrinkage for the common case where the base learners are regression trees. We hypothesize that by varying the shrinkage applied to the prediction of the examples in each individual leaf of the regression tree base learners, we will be able to decrease convergence time without sacrificing resilience to overfitting.

\section{Related Work}
\label{sec:RelatedWork}
Boosting finds its roots in a question originally posed by Kearns and Valient in 1988, is weak learnability equivalent to strong learnability\cite{Kearns:1988} \cite{KearnsValient:1989}? In other words, if you have a way to learn well enough to beat random guessing, is it inherently true that a strong learner, capable of arbitrarily low error, for that same problem exists? Schapire successfully proved this equivalence in 1990 by proposing and proving the correctness of a polynomial time boosting model he termed \textit{The hypothesis boosting mechanism} \cite{StrengthOfWeak}. 

After Schapire's compelling proof that weak and strong learnability are in fact equivalent, researchers bagan working to improve upon his boosting algorithm. The first practical application of the early boosting algorithms came out of the work of Drucker, Schapire, and Simard at AT\&T Bell Labs in 1992. There they applied boosting of neural network base learners to the problem of optical character recognition of handwritten zip codes on USPS letters \cite{OCRDruckerHarrisSchapire}. 

In 1995 Freund and Schapire introduced the AdaBoost algorithm which is hailed as solving many of the practical problems suffered by previous boosting algorithms. The unique idea introduced by Adaboost is the notion of applying higher weights at each iteration to the training examples that were misclassified in previous iterations, forcing the new base learners to focus their efforts on these examples. AdaBoost became famous as an excellent out of the bag approach for classification with exceptional resilience to overfitting \cite{adaboost}.  However, the details of why exactly AdaBoost worked were unknown until the year 1998 when Friedman, Hastie, and Tibshirani explored the algorithm from an in depth statistical viewpoint. They found that AdaBoost a specialized additive model, and applied knowledge from the long history of statistical additive function approximation to gain a better understanding of AdaBoost and boosting in general \cite{friedman2000}. 

With an increased theoretical statistical understanding of boosting now available, Friedman developed a generalized stagewise additive function approximation boosting model termed the gradient boosting machine in 1999, which he later extended to include a stochastic subsampling approach in 2002 \cite{2001Friedman} \cite{2002Friedman}. Gradient Boosting Machines will be explored is great detail in the following sections as extending this model is the focus of the proposed research.

Since their introduction to the machine learning and data mining communities in 1999, gradient boosting machines have found applications in a variety of fields for both classification and regression tasks. Most recently ecology researchers have found great interest in gradient boosting machines, particularly the varient of them that utilizes classification or regression trees as base learners. In 2007, Glenn De'ath extended the R package gbm, creating a new package gbmplus that implements a varient algorithm he terms Aggregated Boosted Trees (ABT). The idea behind ABTs is to perform cross validation to determine an optimal number of iterations for the boosting, then save the models built during cross validation chopping them off at the optimal number of iterations found. To make a prediction, the predictions of all of these boosted trees are computed then averaged. It was found that this approach lead to improved accuracy over gbm alone \cite{death2007ABT}.

Another group of ecological researchers Jane Elith and John Leathwick have also been applying boosted regression trees to their work. One such problem involves predicting whether or not a particular species off eel will be present in unsampled Australian rivers based upon measured environmental factors \cite{ecological}.  Elith and Leathwick implemented their own extensions to the functions in the gbm package in their dismo package in 2015, including a very useful function known as gbm.step. This function implements a cross validation based approach to estimate the number of trees that should be built to reach optimal generalization error, an approach that we will mimic in our experiment \cite{elith2015boosted}. 


\section{The Gradient Boosting Machine}
\label{sec:GradientBoostingMachine}
In supervised learning, our goal is to find an approximation \(\hat{F}\) of an unknown function \(F: \vec{x} \rightarrow y\) that maps data instances \(\vec{x}\) to a set of response variables \(y\) and best minimizes the expected value of some loss function \( \Psi(y, F(\vec{x})) \). 
Friedman's gradient boosting machine achieves this by iteratively constructing a strong learner that approximates \(F\) from many weak learners also referred to as base learners. \(h(\vec{x})\) is the standard notation for the generic base learner, which takes an instance \(\vec{x}\) and predicts the value of its response variable. In each iteration a new base learner, such as a short regression tree, is trained to fit the errors made by the function approximation so far. This training is based upon an extremely common numerical minimization method known as steepest gradient descent \cite{2012ridgeway} \cite{2001Friedman} . 
However, unlike most applications of steepest descent, Friedman's Gradient Boost algorithm computes the negative gradient \(\vec{g}\) in the space of the estimated function itself, not in the space of a finite set of parameters that define the function. By framing the problem in this way, the function \(\hat{F}\) is not limited to a set of functions definable by a finite set of parameters, but rather is defined by a potentially infinite set of parameters, one for each possible value \(\vec{x}\). Obviously, it is impossible to actually compute the gradient and apply steepest-descent in this potentially infinite dimensional function space, but it is possible to perform steepest-descent with respect to the finite space of training examples \(D\) \cite{2012ridgeway} \cite{2001Friedman}. 

The negative gradient in this restricted subset of function space defines the direction of steepest descent in the loss function for the training examples. Thus by updating the function \(\hat{F}\) directly by this negative gradient, we would move closer to the minimum values of the loss function \(\Psi\) for the examples in the training dataset. Of course this is not quite the goal, instead we would like to be able to generalize to all possible data. To accomplish this we instead train a base learner to predict the negative gradient of the loss function at each step, then update our function with this model's prediction. Friedman's general Gradient Boosting Machine, extended to include his later ideas of subsampling the training data and applying a constant shrinkage to improve generalization is provided in Algorithm \ref{alg:GeneralGradientBoost} \cite{2012ridgeway} \cite{2001Friedman} \cite{2002Friedman}. 
%---------------------------------------------------------------
\begin{figure}[t]
	\removelatexerror
	\begin{algorithm}[H]
		\hspace{1 mm} \\\hrule\hspace{1 mm} \\
		\SetKwInOut{Input}{input}
		\SetKwInOut{Output}{output}
		
		\Input{Training Dataset: \(D = (x_i, y_i),\, i = 1...N\) \newline
			Bag Fraction: \(bf \, \epsilon \, [0, 1] \) \newline
			Shrinkage: \(v \, \epsilon \, [0, 1] \) \newline
			Number of Base Learners: \(M \) \newline
			Loss Function: \(\Psi\) e.g. RMSE \newline
			Choice of Base Learner: \(h(\vec{x})\) e.g. regression trees \newline
		}
		\Output{A function \(\hat{F}(\vec{x})\) that minimizes the expected value of \(\Psi(y, F(\vec{x})) \) }
		\hspace{1 mm} \\\hrule\hspace{1 mm} \\
		Initialize the  approximation of \(\hat{F}\)
		\begin{equation}
		\hat{F}_0(\vec{x}) = argmin_\rho \sum_{i=1}^{N}\Psi(y_i, \rho) 
		\end{equation}
		\For{\(m\leftarrow 1\) to \( M\) }
		{
			Select a random subsample, \(S_m\), of size \(\tilde{N}\) from the training data without replacement.
			\begin{equation}
			S_m \subset D, \ \  |S_m| = \tilde{N} =  bf  \cdot  N 
			\end{equation}
			Approximate the negative gradient \(\vec{g}_m\) of \(\Psi(y_i, \hat{F}_{m-1} (\vec{x}) ) \) with respect to \(\hat{F}_{m-1} (\vec{x})\).
			\begin{equation}
			g_{m,i} = -\frac{\partial}{\partial \hat{F}_{m-1}(\vec{x_i})} \Psi(y_i, \hat{F}_{m-1}(\vec{x_i})), \, \vec{x}_i \, \epsilon \, S_m
			\label{eq:NegativeGradient}
			\end{equation}
			Train a new base learner \(h_m(\vec{x})\) to predict \(\vec{g}_m\) and fit the least squares.
			\begin{equation}
			\beta_m, h_m(\vec{x}) = argmin_{\beta, h(\vec{x})}\sum_{\vec{x}_i \, \epsilon \, S_m}[ g_{m,i} - \beta h(\vec{x}_i)]^2
			\label{eq:NewBaseLearner}
			\end{equation}		
			Solve for the optimal coefficient \(\rho\) that minimizes \(\Psi\).
			\begin{equation}
			p_m = argmin_\rho \sum_{\vec{x}_i \, \epsilon \, S_m}\Psi(y_i, \hat{F}_{m-1}(\vec{x}_i) + \rho h(\vec{x}_i))
			\label{eq:ScalingCoefficent}
			\end{equation}
			Update your approximation of \(\hat{F}\), scaled by the shrinkage v
			\begin{equation}
			\hat{F}_m(\vec{x}) = \hat{F}_{m-1}(\vec{x}) + v  \cdot  \rho_m h_m(\vec{x})
			\label{eq:UpdateStep}
			\end{equation}
		}
		\caption{Friedman's Gradient Boost Algorithm  \cite{2001Friedman} \cite{GBMTut} \cite{2002Friedman} \cite{death2007ABT}}
		\hspace{1 mm} \\\hrule\hspace{1 mm} \\\hspace{1 mm} \\
		\label{alg:GeneralGradientBoost}
	\end{algorithm}

\end{figure}
%---------------------------------------------------------------
\section{Simplifying Assumptions}
\label{sec:SimplifyingAssumptions}
For the purposes of the current research, we will consider only the case where \(\Psi\) is the squared error function and the goal is to predict a real valued response variable. Without loss of generality we will add a coefficient of \(\frac{1}{2}\) to \(\Psi\) so that the negative of the partial derivative of \(\Psi\) with respect to the predicted value of an instance \(\vec{x}_i\) is simply the residual of that prediction. In this case the component wise calculation of the negative gradient (Equation \ref{eq:NegativeGradient}) becomes Equation \ref{eq:SquaredErrorGradient}.

\begin{equation}
g_{m,i} = -\frac{\partial}{\partial \hat{F}_{m-1}(\vec{x_i})} \frac{(y_i - \hat{F}_{m-1}(\vec{x_i}))^2}{2} = (y_i - \hat{F}_{m-1}(\vec{x_i}))
\label{eq:SquaredErrorGradient}
\end{equation}

As mentioned in the introduction, the proposed variable shrinkage scheme is specially designed for the common case where the base learners are regression trees, we define the notation for regression trees in Figure \ref{fig:regressionTreeFig} below.

When building the trees, we define the next best split as the split that leads to the largest reduction in squared error on the training examples. Thus the least squares coefficient \(\beta\) in Equation \ref{eq:NewBaseLearner} will always be 1, and since we're using the squared error loss function so will the optimal coefficient \(\rho\) in Equations \ref{eq:ScalingCoefficent} and \ref{eq:UpdateStep}. 


Thus, for the case where the base learners are regression trees and the loss function is the squared error, the update step in Equation \ref{eq:UpdateStep} can be replaced by Equation \ref{eq:UpdateStep2}

\begin{equation}
\hat{F}_m(\vec{x}) = \hat{F}_{m-1}(\vec{x}) + v  \cdot \sum_{j=1}^{J}b_{m,j}I(\vec{x} \, \epsilon \, R_{m,j})
\label{eq:UpdateStep2}
\end{equation}

The choice of regression tree base learners introduces two additional parameters to the gradient boosting algorithm, namely the maximum number of splits in each tree (aka interaction depth) and the minimum number of observations in each leaf node (leaf size). These parameters work together to regularize the complexity of the regression trees, and in general these parameters are chosen to ensure short trees and stout leaves to weaken the predictive power of each individual base learner which ultimately strengthens the boosted model's ability to generalize \cite{ecological}. 

Note that although we restrict the current study to regression tasks using the squared error loss function, the variable shrinkage scheme defined and tested in the following sections could easily be applied to any of the loss functions originally defined by Friedman in \cite{2001Friedman} \cite{2002Friedman}, and most famously implemented in the gbm R package originally written by Ridgeway \cite{2012ridgeway}. Please see these references for information on the many specialized derivations of Algorithm \ref{alg:GeneralGradientBoost} for various learning tasks and to better understand the reasoning behind the equations defined in this section.
%---------------------------------------------------------------
\begin{figure}[t]
	
	\begin{equation}
	h_m(\vec{x}) = \sum_{j=1}^{J}b_{m,j}I(\vec{x} \, \epsilon \, R_{m,j})
	\end{equation}
	Where 
	\begin{center}
		\(J =\) the number of terminal nodes (leaves) in the tree 
	\end{center}
	\begin{center}
		\(b_{m,j} =\) Prediction made for all instances in \(R_{m,j}\).\\ 
		For squared error, \(b_{m,j} =  avg_{x_i \epsilon R_{m,j}}( g_{m, i}) \) 
	\end{center}
	\begin{center}
		\(R_{m,j} = \) The subset of instances \(\vec{x} \, \epsilon \, S_m\) \\
		that are predicted by the \(j^{th}\) terminal node.
	\end{center}
	
	\[ I(\alpha) = \begin{cases} 
	1 & \alpha \text{ is true} \\
	0 & \alpha \text{ is false}  
	\end{cases}
	\]
	\caption{Notation for Regression Tree Base Learners}
	\label{fig:regressionTreeFig}
\end{figure}	
%---------------------------------------------------------------
\section{Variable Shrinkage for Regression Tree Base Learners}
\label{sec:VariableLR}
The use of regression trees presents an interesting possibility of a simple, yet elegant adaptation method. Since the trees themselves can be seen as a summation of individual prediction terms, one for each leaf, a natural adaptation scheme is to compute a different shrinkage for each leaf node in each base learner. Specifically a simple linear mapping can be used to ensure that the lower the number of examples in a given leaf node, the lower the computed shrinkage. We hypothesize that this will encourage the training to learn rapidly from the most typical training examples, which will fall into large leaf nodes along with their similar peers, while diminishing the impact of outlying and noisy examples, which will be isolated by the regression tree's splitting algorithm. Overall we expect this scheme to lead to faster convergence time while strongly discouraging overfitting.

To this end we will alter Algorithm 1 to take as input both a minimum and maximum shrinkage \(v_{min}\) and \(v_{max}\), instead of the constant shrinkage \(v\). The following equation will then be used to compute the shrinkage for each of the J leaves in Equation \ref{eq:UpdateStep2}.
	
\begin{equation}
v_{m,j} = \frac{|R_{m,j}|}{{|S_m|}}(v_{max} - v_{min})  + v_{min}
	\label{eq:adaptRule1}
\end{equation}

This equation maps the range of possible leaf node sizes \([\ 0, \tilde{N}\ ]\) to the range of possible shrinkage values \([\ v_{min}, v_{max}\ ]\). Note that even when the regression trees are built with a nonzero minimum number of observations in each leaf node, the missing value branches of the trees can still have any number of examples in them, including zero if no missing values existed in the training data. Thus zero is still used as the minimum leaf size for the purposes of shrinkage calculation regardless of the minimum number of observations parameter. 

Using equation \ref{eq:adaptRule1}, the update step (Equation \ref{eq:UpdateStep2}) becomes Equation \ref{eq:revisedUpdateStep}.

\begin{equation}
	\hat{F}_m(\vec{x}) = \hat{F}_{m-1}(\vec{x}) + \rho_m\sum_{j=1}^{J}v_{m,j}  \cdot  b_{m,j}I(\vec{x} \, \epsilon \, R_{m,j})
	\label{eq:revisedUpdateStep}
\end{equation}
	
\section{Implementation}
We implemented the Friedman's gradient boosting machine with the option to utilize either the standard constant shrinkage or our proposed variable shrinkage scheme as defined in Sections \ref{sec:GradientBoostingMachine}, \ref{sec:SimplifyingAssumptions}, and \ref{sec:VariableLR}. The core algorithm implementation is an extension of the very minimal JSGBM project found here \cite{jsgbm}. The additions to the original project include support for missing values and splits on categorical variables, optimization of the splitting algorithm, calculation of predictor relative influence, a cross validation based method for selecting the optimal number of trees, and support for De'ath's aggregated boosted trees as discussed in Section \ref{sec:RelatedWork}. All code implemented for this paper can be found at \cite{myGithub}.

The algorithm for selecting the optimal number of trees is based on the gbm.step function of the dismo R package which is described in \cite{ecological}. The psuedocode for our modified implementation in provided in Algorithm \ref{alg:OptNumOfTrees}.

\begin{algorithm}[]
	\hspace{1 mm} \\\hrule\hspace{1 mm} \\
%	\rule{\textwidth}{2pt}
	\SetKwInOut{Input}{input}
	\SetKwInOut{Output}{output}
	
	\Input{Number of Folds: \(K = 5\) \newline
		Step Size: \(S_{size} = 500\): \newline
		Steps Past Minimum: \(S_{pm} = 3 \) \newline
		Max Number of Trees: \(M = 150,000\) \newline
	}
	\Output{Estimated number of trees at which generalization error is minimized} 
	\hspace{1 mm} \\\hrule\hspace{1 mm} \\
	
	Create \(K\) training/validation set pairs as in normal K-fold cross validation. For each pair, create a new empty GBM model. \\
	\hspace{1 mm} \\
	
	\(
	S_{remaining} \leftarrow S_{pm}
	\) (Initialize number of steps remaining) 	\hspace{1 mm} \\\hspace{1 mm} \\

	\While{ \textsf{\upshape NumberOfTrees \textless } \(M\) \textsf{\upshape and} \(S_{remaining}\) $\textsf{\upshape \textgreater \  0}$}
	{
		
		Add \(S_{size}\) trees to each of the \(K\) GBM models.
		\hspace{1 mm} \\
		
		\(avgRMSE \leftarrow \) Evaluate the new validation RMSE of each model, and average them.
		
		\uIf{\(avgRMSE < minAvgRMSE\)}{
			\hspace{1 mm} \\
			\(S_{remaining} \leftarrow S_{pm}\) \\
			\(minAvgRMSE \leftarrow avgRMSE\)
			}
		
		\uElse{\(S_{remaining} \leftarrow S_{remaining} - 1\)}
	}
	\caption{Find Optimal Number of Trees using Cross Validation}
	\hspace{1 mm} \\\hrule\hspace{1 mm} \\\hspace{1 mm} \\
	\label{alg:OptNumOfTrees}
\end{algorithm}


\section{Datasets}
\label{sec:datasets}
Four real world datasets with natural regression tasks were selected for use in this study. Each of these can be found in the UC Irvine Machine Learning Repository \cite{Lichman:2013}. This section introduces each of these in turn. Note the parenthesized minimal name found in the subsection titles will be used to refer to these datasets throughout the results and discussion sections.

\subsection{Combined Cycle Power Plant (powerPlant)}
This dataset contains 9568 examples collected over 6 years from a power plant set work at full load. Each instance consists of 4 real valued predictors; namely the  Temperature (AT), Ambient Pressure (AP), Relative Humidity (RH) and Exhaust Vacuum (V). The target response variable is the net hourly electrical energy output (EP) of the plant \cite{powerPlantDataset}. The powerPlant dataset was subjected to extensive cleaning and preprocessing prior to donation to the UCI repository. This preprocessing procedure filtered out all nonsensical outliers and diminish the noise that resulted from electrical disturbance interfering with the signal \cite{powerPlantCiteRequest1} \cite{powerPlantCiteRequest2}.

\subsection{Airfoil Self-Noise (nasa)}
This data originated from a 1989 NASA experiment in which different size air foils (wing shapes) were placed in a wind tunnel and subjected to various free-stream velocities (wind speed prior to hitting the air foil) and angles of attack (direction of the wind). The regression goal is to predict the scaled sound pressure level in decibels of an air foil given the free-stream velocity (meters/second), angle of attack (degrees), frequency (Hertz), chord length (meters), and suction side displacement thickness (meters) \cite{airFoilDataset}. More information on the creation and properties of this dataset can be found in the following relevant papers \cite{nasaCiteRequest1} \cite{nasaCiteRequest2} \cite{nasaCiteRequest3}.

\subsection{Bike Sharing (bikeSharing)}
Hadi Fanaee from the Laboratory of Artificial Intelligence and Decision Support (LIAAD), University of Porto pulled together weather information and holiday information and integrated it with publically available trip data provided by Capital Bikeshare to generate this intriguing dataset consisting of almost two years of bike rental data aggregated both by day and by hour. Each instance contains attributes providing information about the day [and hour] that the rentals occurred. This includes the year, season, month, day, hour, whether it was a holiday, a weekday, or a working day. All of the time attributes are treated as categorical predictors in our experiment. In addition the weather conditions for each instant in time are provided including the precipitation type, temperature, humidity, and wind speed. Finally the count of bikes rented during each time period is provided, which will be used as the target variable in our experiment \cite{bikeSharingDataset} \cite{bikeSharingCiteRequest}. 

For our purposes we will look only at the daily data which limits the dataset to 731 instances rather than 17381 instances in the hourly version. No normalization or data cleaning has been applied to this dataset. This dataset is of course naturally quite noisy since people do not follow any hard and fast rules when deciding whether or not to go on a bike ride so the rental counts can vary considerably even for identical values of all the predictors.

\subsection{Communities and Crime (crimeCommunities)}
Lastly we will look at the Communities and Crime dataset which contains 122 predictors for the ratio of violent crime to population in 1994 communities throughout the United States, where violent crimes are defined as murder, rape, robbery, and assault. Each example is comprised of data from the 1990 US Census, law enforcement data from the 1990 US LEMAS survey, and crime 
data from the 1995 FBI UCR. Some examples of the predictors provided are the household size, percent unemployed, divorce rates, ratio of police offices to population, number of different kinds of drugs seized by the police, average rent costs, percentage of variance races and ethnicities, and many more aspects of the community and local police force. Note that in this dataset all of the variables have been normalized to the range \([\ 0, 1\ ]\) and missing values are prevalent for many of the predictors. \cite{crimeCommunitiesDataset} \cite{crimeCommunitiesCiteRequest1} \cite{crimeCommunitiesCiteRequest2} \cite{crimeCommunitiesCiteRequest3} \cite{crimeCommunitiesCiteRequest4} \cite{crimeCommunitiesCiteRequest5}.

\section {Experiment}
In order to compare model performance of the original algorithm with our proposed shrinkage adaption scheme, we developed an experiment in which 2048 total sets of parameters were tested on the four datasets described in Section \ref{sec:datasets}. 

All parameters that affect the execution of Algorithm \ref{alg:OptNumOfTrees}, which is the core training procedure used in all of our tests, are shown in Table \ref{tab:findOptTreeParam}. Two additional parameters are listed that were not introduced with algorithm itself but are important to note. These indicate that the running time and memory usage for during training was limited to 1.5 hours and 20 GB respectively. These values were exceeded only a handful of times when using the lowest constant learning rate and the highest number of splits. In the rare case that a test exceeded these limits the training stopped and the application continued with collecting and saving metadata about the constructed models, just as though the training stopped due to the normal stopping condition.

Gradient boosting machines with regression tree base learners are traditionally defined by the number of trees to be built and 4 additional parameters; the bag fraction, shrinkage, minimum number of examples in each leaf, and the maximum number of splits in each tree. To support our variable shrinkage scheme we must add one additional parameter since we require both a minimum and a maximum learning rate. All values that were considered for each of these parameters are provided in Table \ref{tab:parameters}. Taking all possible combinations of these parameters leads to 512 parameter sets with constant shrinkage, and 1536 possibilities with variable shrinkage for a total of 2048 individual tests for each dataset. 

For a single run on a given dataset, all 2048 parameters were evaluated using the same 80\% training set / 20\% test set split. Selection of the test set examples was performed uniformly at random. The exact procedure used to evaluate a single set of parameters is provided in Algorithm \ref{alg:tuningProcedure}.
%---------------------------------------------------------------
\begin{table}
	\centering
	\begin{tabular}{ | l || c |}
		\hline
		\rule{0pt}{2ex} System CPU & Intel i5-3570 \\ \hline
		\rule{0pt}{2ex} System Memory & 32 GB \\ \hline
		\rule{0pt}{2ex} Maximum Running Time & 1.5 hours \\ \hline
		\rule{0pt}{2ex} Maximum Memory Usage & 20 GB \\ \hline
	\end{tabular}
	\caption{CPU and Memory Hardware and Limits}
	\label{tab:executionLimits}
\end{table}
\begin{table}
	\centering
	\begin{tabular}{ | l || c |}
	\hline
	\rule{0pt}{2ex}	Maximum Number of Trees & 150,000 \\ \hline
	\rule{0pt}{2ex}	Number of CV Folds & 5 \\ \hline
	\rule{0pt}{2ex}	CV Step Size & 500 \\ \hline
	\rule{0pt}{2ex}	Max CV Steps w/o improving CV Error	& 3 \\ \hline
	\end{tabular}
	\caption{Execution limits and Parameters for Finding Optimal Number of Trees}
	\label{tab:findOptTreeParam}
\end{table}
%---------------------------------------------------------------
\begin{table}
	\centering
		\begin{tabular}	{ | l || c |}
			\hline
			Bag Fraction & 0.25, 0.50, 0.75, 1 \\ \hline
			Min Examples In Node & 1, 10, 75, 150 \\ \hline
			Max Number Of Splits & 1, 2, 4, 8, 16, 32, 64, 128 \\ \hline
			Constant: Shrinkage & 0.1, 0.01, 0.001, 0.0001 \\ \hline
			Variable: Minimum Shrinkage & 0.01, 0.001, 0.0001 \\ \hline
			Variable: Maximum Shrinkage & 0.1, 0.4, 0.7, 1 \\
			\hline
			
		\end{tabular}
		\caption{Execution limits and Parameters for Finding Optimal Number of Trees}
		\label{tab:parameters}
\end{table}
%---------------------------------------------------------------

\begin{algorithm}[]
	\hspace{1 mm} \\\hrule\hspace{1 mm} \\
	\begin{enumerate}
		\item Use Algorithm \ref{alg:OptNumOfTrees} to find the optimal number of trees. Simultaneously train a GBM using the entire training set, stop training when Algorithm \ref{alg:OptNumOfTrees}'s stopping condition is reached.
		\item Build an Aggregated Boosted Tree from the K GBMs built for cross validation.
		\item Collect and save meta-data about the run. This includes the optimal number of trees, the error values for each possible number of trees, the mean and standard deviation values for the actual number of splits and leaf sizes of each tree, the predictions made for each example at the optimal number of trees, and the average computed shrinkage values for each example and for each tree.
		\item Compute the relative influences of the predictors for both the all training data GBM and the aggregated boosted tree.
		\item Record the total amount of time taken to perform all of these steps.
	\end{enumerate}
	\caption{Procedure for a Single Test.}
	\hspace{1 mm} \\\hrule\hspace{1 mm} \\\hspace{1 mm} \\
	\label{alg:tuningProcedure}
\end{algorithm}%

\begin{figure}[!t]

\end{figure}
%---------------------------------------------------------------
\section{Results}

For each dataset, we have sorted all parameter sets by the cross validation root mean squared error (cv RMSE) at the optimal number of trees. For both constant and variable shrinkages, the five sets of parameters leading to the lowest average cv RMSE are provided for each dataset in Tables III through X. Note the lighter shaded cells indicate the minimum value in each row, while the darker shade indicates the maximum value.

The topmost sections of these tables contain the parameter values themselves (columns are sorted by CV Error in ascending order). 

In the middle portion of the table we have the running time, average cross validation error, average performance of the GBMs built with all training data on the test set (ATD Test RMSE), average performance of the Aggregated Boosted Trees on the test set (ABT Test RMSE), as well as the average optimal number of trees found across the five runs. 

The bottom most sections of these tables contain the mean and standard deviation values of the actual leaf sizes and number of splits across all trees. In the case of variable shrinkage we additionally provide the average shrinkage applied across all of the examples. 

It's important to notice that the trees cannot always be grown to the maximum number of splits or to the point of reaching the minimum leaf size. The tree building algorithm terminates when it can no longer find a split that decreases the overall squared error of the training instances involved in the split. Of course if the min leaf size is set to 1, the algorithm will always be able to improve by placing an example in a node by itself. When the min leaf size is greater than 1 however it is possible to that no possible split improves the error and the algorithm will stop with less than the maximum number of splits and nodes that still have more than twice the minimum leaf size.


In figures \todo{} we have

\import{Z:/GBMWithVariableShrinkage/parameterTuning/5/}{entireResultsSection}




%---------------------------------------------------------------
Next we provide the error curves

\section{Discussion}
Analysis and stuff
%---------------------------------------------------------------
\section{Conclusion}
The conclusion goes here.

\newpage
% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section
\bibliographystyle{ieeetran}
\bibliography{references}

% that's all folks
\end{document}


