\documentclass[9pt, conference]{IEEEtran}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)

% *** CITATION PACKAGES ***
%
\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/tex-archive/info/epslatex/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/




% *** FLOAT PACKAGES ***
%
\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Do not use the stfloats baselinefloat ability as IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/dblfloatfix/




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/url/
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{placeins}
\usepackage{fancyvrb}
\usepackage[bf]{caption2}
\usepackage{comment}
\usepackage{hhline}
\usepackage{algorithm2e}

\usepackage{filecontents,lipsum}
\usepackage{todonotes}
\usepackage{xcolor}% you could also use the color package
\usepackage{colortbl}
\usepackage{lipsum}
\usepackage{verbatim}
\usepackage{import}
\usepackage{tabularx, booktabs}
\makeatletter
\newcommand{\removelatexerror}{\let\@latex@error\@gobble}
\makeatother


\newcolumntype{Y}{>{\centering\arraybackslash}X}

%\makeatletter
%\@dblfptop 0pt
%\makeatother

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Gradient Boosting Regression Trees with Variable Shrinkage}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Austin Barket}
\IEEEauthorblockA{Department of Computer Science\\
The Pennsylvania State University at Harrisburg\\
Middletown, PA 17057\\
Email: amb6470@psu.edu}
\and
\IEEEauthorblockN{Dr. Jeremy Blum}
\IEEEauthorblockA{Department of Computer Science\\
The Pennsylvania State University at Harrisburg\\
Middletown, PA 17057\\
Email: jjb24@psu.edu}
}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
We explore the effect of variable shrinkage (learning rate) on gradient boosting machines that utilize regression trees as the base learners. Conventionally, research and implementations of gradient boosting machines have used only constant shrinkage that is applied uniformly to the predictions of all base learners. Results show that the use of variable shrinkage results in models with competitive error to those built using constant shrinkage. On average the use of variable shrinkage leads to a considerable decrease in running time and increased robustness to variations in the model parameters.
\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}

Boosting is a machine learning technique that combines learning algorithms that barely beat random guessing, known as a weak or base learners, into a single model with significantly improved accuracy or lower error rates over any of its constituent parts \cite{StrengthOfWeak}  \cite{BoostingSurvey}. 

The gradient boosting machine (GBM), originally introduced by Friedman in 1999 is a general boosting framework that leverages the steepest descent numerical optimization method to iteratively train base learners to address the errors made by those before them \cite{2001Friedman}.

Production implementations of gradient boosting machines such as the gbm package in R have found remarkable traction among researchers in a wide variety of fields including robotics and ecology \cite{GBMTut} \cite{ecological}. Interestingly these packages generally implement Friedman's Gradient Boost algorithm as it was originally defined \cite{2012ridgeway}, leaving some definite opportunity for research into algorithmic tweaks to improve performance. 

A particular component of the algorithm that has not been explored to date is the learning rate, commonly referred to as shrinkage in the case of GBMs. Shrinkage is traditionally implemented as a constant parameter to the model. After each iteration the new base learner's predictions are scaled by the shrinkage as a form of regularization. 

This research presents and evaluates a new way to think about shrinkage for the common case where the base learners are regression trees. We compared our proposed variable shrinkage scheme against the traditional constant shrinkage scheme using four real world datasets with natural regression tasks. Our results show that the use of variable shrinkage results in models with competitive error to those built using constant shrinkage. On average the use of variable shrinkage leads to a considerable decrease in running time and increased robustness to variations in the model parameters without sacrificing resilience to over fitting. 

\section{Related Work}
\label{sec:RelatedWork}
Boosting finds its roots in a question originally posed by Kearns and Valient in 1988, is weak learnability equivalent to strong learnability\cite{Kearns:1988} \cite{KearnsValient:1989}? In other words, if you have a way to learn well enough to beat random guessing, is it inherently true that a strong learner, capable of arbitrarily low error, for that same problem exists? Schapire successfully proved this equivalence in 1990 by proposing and proving the correctness of a polynomial time boosting model he termed \textit{The hypothesis boosting mechanism} \cite{StrengthOfWeak}. 

After Schapire's compelling proof that weak and strong learnability are in fact equivalent, researchers bagan working to improve upon his boosting algorithm. The first practical application of the early boosting algorithms came out of the work of Drucker, Schapire, and Simard at AT\&T Bell Labs in 1992. There they applied boosting of neural network base learners to the problem of optical character recognition of handwritten zip codes on USPS letters \cite{OCRDruckerHarrisSchapire}. 

In 1995 Freund and Schapire introduced the AdaBoost algorithm which is hailed as solving many of the practical problems suffered by previous boosting algorithms. The unique idea introduced by Adaboost is the notion of applying higher weights at each iteration to the training examples that were misclassified in previous iterations, forcing the new base learners to focus their efforts on these examples. AdaBoost became famous as an excellent out of the bag approach for classification with exceptional resilience to overfitting \cite{adaboost}.  However, the details of why exactly AdaBoost worked were unknown until the year 1998 when Friedman, Hastie, and Tibshirani explored the algorithm from an in depth statistical viewpoint. They found that AdaBoost is a specialized additive model, and applied knowledge from the long history of statistical additive function approximation to gain a better understanding of AdaBoost and boosting in general \cite{friedman2000}. 

With an increased theoretical statistical understanding of boosting now available, Friedman developed a generalized stagewise additive function approximation boosting model termed the gradient boosting machine in 1999, which he later extended to include a stochastic subsampling approach in 2002 \cite{2001Friedman} \cite{2002Friedman}. Gradient Boosting Machines will be explored is great detail in the following sections as extending this model is the focus of our current research.

Since their introduction to the machine learning and data mining communities in 1999, gradient boosting machines have found applications in a variety of fields for both classification and regression tasks. Most recently ecology researchers have found great interest in gradient boosting machines, particularly the varient of them that utilizes classification or regression trees as base learners. In 2007, Glenn De'ath extended the R package gbm, creating a new package gbmplus that implements a varient algorithm he terms Aggregated Boosted Trees (ABT). The idea behind ABTs is to perform cross validation to determine an optimal number of iterations for the boosting, then save the models built during cross validation chopping them off at the optimal number of iterations found. To make a prediction, the predictions of all of these boosted trees are computed then averaged. It was found that this approach lead to improved accuracy over gbm alone \cite{death2007ABT}.

Another group of ecological researchers Jane Elith and John Leathwick have also been applying boosted regression trees to their work. One such problem involves predicting whether or not a particular species off eel will be present in unsampled Australian rivers based upon measured environmental factors \cite{ecological}.  Elith and Leathwick implemented their own extensions to the functions in the gbm package in their dismo package in 2015, including a very useful function known as gbm.step. This function implements a cross validation based approach to estimate the number of trees that should be built to reach optimal generalization error, an approach that we will mimic in our experiment \cite{elith2015boosted}. 


\section{The Gradient Boosting Machine}
\label{sec:GradientBoostingMachine}
In supervised learning, our goal is to find an approximation \(\hat{F}\) of an unknown function \(F: \vec{x} \rightarrow y\) that maps data instances \(\vec{x}\) to a set of response variables \(y\) and best minimizes the expected value of some loss function \( \Psi(y, F(\vec{x})) \). 
Friedman's gradient boosting machine achieves this by iteratively constructing a strong learner that approximates \(F\) from many weak learners also referred to as base learners. \(h(\vec{x})\) is the standard notation for the generic base learner, which takes an instance \(\vec{x}\) and predicts the value of its response variable. In each iteration a new base learner, such as a short regression tree, is trained to fit the errors made by the function approximation so far. This training is based upon an extremely common numerical minimization method known as steepest gradient descent \cite{2012ridgeway} \cite{2001Friedman} . 
However, unlike most applications of steepest descent, Friedman's Gradient Boost algorithm computes the negative gradient \(\vec{g}\) in the space of the estimated function itself, not in the space of a finite set of parameters that define the function. By framing the problem in this way, the function \(\hat{F}\) is not limited to a set of functions definable by a finite set of parameters, but rather is defined by a potentially infinite set of parameters, one for each possible value \(\vec{x}\). Obviously, it is impossible to actually compute the gradient and apply steepest-descent in this potentially infinite dimensional function space, but it is possible to perform steepest-descent with respect to the finite space of training examples \(D\) \cite{2012ridgeway} \cite{2001Friedman}. 

The negative gradient in this restricted subset of function space defines the direction of steepest descent in the loss function for the training examples. Thus by updating the function \(\hat{F}\) directly by this negative gradient, we would move closer to the minimum values of the loss function \(\Psi\) for the examples in the training dataset. Of course this is not quite the goal, instead we would like to be able to generalize to all possible data. To accomplish this we instead train a base learner to predict the negative gradient of the loss function at each step, then update our function with this model's prediction. Friedman's general Gradient Boosting Machine, extended to include his later ideas of subsampling the training data and applying a constant shrinkage to improve generalization is provided in Algorithm \ref{alg:GeneralGradientBoost} \cite{2012ridgeway} \cite{2001Friedman} \cite{2002Friedman}. 
%---------------------------------------------------------------
\begin{figure}[t]
	\removelatexerror
	\begin{algorithm}[H]
		\hspace{1 mm} \\\hrule\hspace{1 mm} \\
		\SetKwInOut{Input}{input}
		\SetKwInOut{Output}{output}
		
		\Input{Training Dataset: \(D = (x_i, y_i),\, i = 1...N\) \newline
			Bag Fraction: \(bf \, \epsilon \, [0, 1] \) \newline
			Shrinkage: \(v \, \epsilon \, [0, 1] \) \newline
			Number of Base Learners: \(M \) \newline
			Loss Function: \(\Psi\) e.g. RMSE \newline
			Choice of Base Learner: \(h(\vec{x})\) e.g. regression trees \newline
		}
		\Output{A function \(\hat{F}(\vec{x})\) that minimizes the expected value of \(\Psi(y, F(\vec{x})) \) }
		\hspace{1 mm} \\\hrule\hspace{1 mm} \\
		Initialize the  approximation of \(\hat{F}\)
		\begin{equation}
		\hat{F}_0(\vec{x}) = argmin_\rho \sum_{i=1}^{N}\Psi(y_i, \rho) 
		\end{equation}
		\For{\(m\leftarrow 1\) to \( M\) }
		{
			Select a random subsample, \(S_m\), of size \(\tilde{N}\) from the training data without replacement.
			\begin{equation}
			S_m \subset D, \ \  |S_m| = \tilde{N} =  bf  \cdot  N 
			\end{equation}
			Approximate the negative gradient \(\vec{g}_m\) of \(\Psi(y_i, \hat{F}_{m-1} (\vec{x}) ) \) with respect to \(\hat{F}_{m-1} (\vec{x})\).
			\begin{equation}
			g_{m,i} = -\frac{\partial}{\partial \hat{F}_{m-1}(\vec{x_i})} \Psi(y_i, \hat{F}_{m-1}(\vec{x_i})), \, \vec{x}_i \, \epsilon \, S_m
			\label{eq:NegativeGradient}
			\end{equation}
			Train a new base learner \(h_m(\vec{x})\) to predict \(\vec{g}_m\) and fit the least squares.
			\begin{equation}
			\beta_m, h_m(\vec{x}) = argmin_{\beta, h(\vec{x})}\sum_{\vec{x}_i \, \epsilon \, S_m}[ g_{m,i} - \beta h(\vec{x}_i)]^2
			\label{eq:NewBaseLearner}
			\end{equation}		
			Solve for the optimal coefficient \(\rho\) that minimizes \(\Psi\).
			\begin{equation}
			p_m = argmin_\rho \sum_{\vec{x}_i \, \epsilon \, S_m}\Psi(y_i, \hat{F}_{m-1}(\vec{x}_i) + \rho h(\vec{x}_i))
			\label{eq:ScalingCoefficent}
			\end{equation}
			Update your approximation of \(\hat{F}\), scaled by the shrinkage v
			\begin{equation}
			\hat{F}_m(\vec{x}) = \hat{F}_{m-1}(\vec{x}) + v  \cdot  \rho_m h_m(\vec{x})
			\label{eq:UpdateStep}
			\end{equation}
		}
		\caption{Friedman's Gradient Boost Algorithm  \cite{2001Friedman} \cite{GBMTut} \cite{2002Friedman} \cite{death2007ABT}}
		\hspace{1 mm} \\\hrule\hspace{1 mm} \\\hspace{1 mm} \\
		\label{alg:GeneralGradientBoost}
	\end{algorithm}

\end{figure}
%---------------------------------------------------------------
\section{Simplifying Assumptions}
\label{sec:SimplifyingAssumptions}
For the purposes of the current research, we will consider only the case where \(\Psi\) is the squared error function and the goal is to predict a real valued response variable. Without loss of generality we will add a coefficient of \(\frac{1}{2}\) to \(\Psi\) so that the negative of the partial derivative of \(\Psi\) with respect to the predicted value of an instance \(\vec{x}_i\) is simply the residual of that prediction. In this case the component wise calculation of the negative gradient (Equation \ref{eq:NegativeGradient}) becomes Equation \ref{eq:SquaredErrorGradient}.

\begin{equation}
g_{m,i} = -\frac{\partial}{\partial \hat{F}_{m-1}(\vec{x_i})} \frac{(y_i - \hat{F}_{m-1}(\vec{x_i}))^2}{2} = (y_i - \hat{F}_{m-1}(\vec{x_i}))
\label{eq:SquaredErrorGradient}
\end{equation}

As mentioned in the introduction, the proposed variable shrinkage scheme is specially designed for the common case where the base learners are regression trees, we define the notation for regression trees in Figure \ref{fig:regressionTreeFig}.

When building the trees, we define the next best split as the split that leads to the largest reduction in squared error on the training examples. Thus the least squares coefficient \(\beta\) in Equation \ref{eq:NewBaseLearner} will always be 1, and since we're using the squared error loss function so will the optimal coefficient \(\rho\) in Equations \ref{eq:ScalingCoefficent} and \ref{eq:UpdateStep}. 


Thus, for the case where the base learners are regression trees and the loss function is the squared error, the update step in Equation \ref{eq:UpdateStep} can be replaced by Equation \ref{eq:UpdateStep2}

\begin{equation}
\hat{F}_m(\vec{x}) = \hat{F}_{m-1}(\vec{x}) + v  \cdot \sum_{j=1}^{J}b_{m,j}I(\vec{x} \, \epsilon \, R_{m,j})
\label{eq:UpdateStep2}
\end{equation}

The choice of regression tree base learners introduces two additional parameters to the gradient boosting algorithm, namely the maximum number of splits in each tree (aka interaction depth) and the minimum number of observations in each leaf node (leaf size). These parameters work together to regularize the complexity of the regression trees. In general these parameters are chosen to ensure short trees and stout leaves to weaken the predictive power of each individual base learner which ultimately strengthens the boosted model's ability to generalize \cite{ecological}. 

Note that although we restrict the current study to regression tasks using the squared error loss function, the variable shrinkage scheme defined and tested in the following sections could easily be applied to any of the loss functions originally defined by Friedman in \cite{2001Friedman} \cite{2002Friedman}, and most famously implemented in the gbm R package originally written by Ridgeway \cite{2012ridgeway}. Please see these references for information on the many specialized derivations of Algorithm \ref{alg:GeneralGradientBoost} for various learning tasks and to better understand the reasoning behind the equations defined in this section.
%---------------------------------------------------------------
\begin{figure}[t]
	
	\begin{equation}
	h_m(\vec{x}) = \sum_{j=1}^{J}b_{m,j}I(\vec{x} \, \epsilon \, R_{m,j})
	\end{equation}
	Where 
	\begin{center}
		\(J =\) the number of terminal nodes (leaves) in the tree 
	\end{center}
	\begin{center}
		\(b_{m,j} =\) Prediction made for all instances in \(R_{m,j}\).\\ 
		For squared error, \(b_{m,j} =  avg_{x_i \epsilon R_{m,j}}( g_{m, i}) \) 
	\end{center}
	\begin{center}
		\(R_{m,j} = \) The subset of instances \(\vec{x} \, \epsilon \, S_m\) \\
		that are predicted by the \(j^{th}\) terminal node.
	\end{center}
	
	\[ I(\alpha) = \begin{cases} 
	1 & \alpha \text{ is true} \\
	0 & \alpha \text{ is false}  
	\end{cases}
	\]
	\caption{Notation for Regression Tree Base Learners}
	\label{fig:regressionTreeFig}
\end{figure}	
%---------------------------------------------------------------
\section{Variable Shrinkage for Regression Tree Base Learners}
\label{sec:VariableLR}
The use of regression trees presents an interesting possibility of a simple, yet elegant adaptation method. Since the trees themselves can be seen as a summation of individual prediction terms, one for each leaf, a natural adaptation scheme is to compute a different shrinkage for each leaf node in each base learner. Specifically a simple linear mapping can be used to ensure that the lower the number of examples in a given leaf node, the lower the computed shrinkage. We hypothesize that this will encourage the training to learn rapidly from the most typical training examples, which will fall into large leaf nodes along with their similar peers, while diminishing the impact of outlying and noisy examples, which will be isolated by the regression tree's splitting algorithm. Overall we expect this scheme to lead to faster convergence time while strongly discouraging overfitting.

To this end we will alter Algorithm 1 to take as input both a minimum and maximum shrinkage \(v_{min}\) and \(v_{max}\), instead of the constant shrinkage \(v\). The following equation will then be used to compute the shrinkage for each of the J leaves in Equation \ref{eq:UpdateStep2}.
	
\begin{equation}
v_{m,j} = \frac{|R_{m,j}|}{{|S_m|}}(v_{max} - v_{min})  + v_{min}
	\label{eq:adaptRule1}
\end{equation}

This equation maps the range of possible leaf node sizes \([\ 0, \tilde{N}\ ]\) to the range of possible shrinkage values \([\ v_{min}, v_{max}\ ]\). Note that even when the regression trees are built with a nonzero minimum number of observations in each leaf node, the missing value branches of the trees can still have any number of examples in them, including zero if no missing values existed in the training data. Thus zero is still used as the minimum leaf size for the purposes of shrinkage calculation regardless of the minimum number of observations parameter. 

Using equation \ref{eq:adaptRule1}, the update step (Equation \ref{eq:UpdateStep2}) becomes Equation \ref{eq:revisedUpdateStep}.

\begin{equation}
	\hat{F}_m(\vec{x}) = \hat{F}_{m-1}(\vec{x}) + \rho_m\sum_{j=1}^{J}v_{m,j}  \cdot  b_{m,j}I(\vec{x} \, \epsilon \, R_{m,j})
	\label{eq:revisedUpdateStep}
\end{equation}
	
\section{Implementation}
We implemented the Friedman's gradient boosting machine with the option to utilize either the standard constant shrinkage or our proposed variable shrinkage scheme as defined in Sections \ref{sec:GradientBoostingMachine}, \ref{sec:SimplifyingAssumptions}, and \ref{sec:VariableLR}. The core algorithm implementation is an extension of the very minimal JSGBM project found here \cite{jsgbm}. The additions to the original project include support for missing values and splits on categorical variables, optimization of the splitting algorithm, calculation of predictor relative influence, a cross validation based method for selecting the optimal number of trees, and support for De'ath's aggregated boosted trees as discussed in Section \ref{sec:RelatedWork}. All code implemented for this paper can be found at \cite{myGithub}.

The algorithm for selecting the optimal number of trees is based on the gbm.step function of the dismo R package which is described in \cite{ecological}. The psuedocode for our modified implementation in provided in Algorithm \ref{alg:OptNumOfTrees}.

\begin{algorithm}[]
	\hspace{1 mm} \\\hrule\hspace{1 mm} \\
%	\rule{\textwidth}{2pt}
	\SetKwInOut{Input}{input}
	\SetKwInOut{Output}{output}
	
	\Input{Number of Folds: \(K = 5\) \newline
		Step Size: \(S_{size} = 500\): \newline
		Steps Past Minimum: \(S_{pm} = 3 \) \newline
		Max Number of Trees: \(M = 150,000\) \newline
	}
	\Output{Estimated number of trees at which generalization error is minimized} 
	\hspace{1 mm} \\\hrule\hspace{1 mm} \\
	
	Create \(K\) training/validation set pairs as in normal K-fold cross validation. For each pair, create a new empty GBM model. \\
	\hspace{1 mm} \\
	
	\(
	S_{remaining} \leftarrow S_{pm}
	\) (Initialize number of steps remaining) 	\hspace{1 mm} \\\hspace{1 mm} \\

	\While{ \textsf{\upshape NumberOfTrees \textless } \(M\) \textsf{\upshape and} \(S_{remaining}\) $\textsf{\upshape \textgreater \  0}$}
	{
		
		Add \(S_{size}\) trees to each of the \(K\) GBM models.
		\hspace{1 mm} \\
		
		\(avgRMSE \leftarrow \) Evaluate the new validation RMSE of each model, and average them.
		
		\uIf{\(avgRMSE < minAvgRMSE\)}{
			\hspace{1 mm} \\
			\(S_{remaining} \leftarrow S_{pm}\) \\
			\(minAvgRMSE \leftarrow avgRMSE\)
			}
		
		\uElse{\(S_{remaining} \leftarrow S_{remaining} - 1\)}
	}
	\caption{Find Optimal Number of Trees using Cross Validation}
	\hspace{1 mm} \\\hrule\hspace{1 mm} \\\hspace{1 mm} \\
	\label{alg:OptNumOfTrees}
\end{algorithm}


\section{Datasets}
\label{sec:datasets}
Four real world datasets with natural regression tasks were selected for use in this study. Each of these can be found in the UC Irvine Machine Learning Repository \cite{Lichman:2013}. This section introduces each of these in turn. Note the parenthesized minimal name found in the subsection titles will be used to refer to these datasets throughout the results and discussion sections.

\subsection{Combined Cycle Power Plant (Power Plant)}
This dataset contains 9568 examples collected over 6 years from a power plant set work at full load. Each instance consists of 4 real valued predictors; namely the  Temperature (AT), Ambient Pressure (AP), Relative Humidity (RH) and Exhaust Vacuum (V). The target response variable is the net hourly electrical energy output (EP) of the plant \cite{powerPlantDataset}. The Power Plant dataset was subjected to extensive cleaning and preprocessing prior to donation to the UCI repository. This preprocessing procedure filtered out all nonsensical outliers and diminished the noise that resulted from electrical disturbance interfering with the signal \cite{powerPlantCiteRequest1} \cite{powerPlantCiteRequest2}.

\subsection{Airfoil Self-Noise (NASA)}
This data originated from a 1989 NASA experiment in which different size air foils (wing shapes) were placed in a wind tunnel and subjected to various free-stream velocities (wind speed prior to hitting the air foil) and angles of attack (direction of the wind). The regression goal is to predict the scaled sound pressure level in decibels of an air foil given the free-stream velocity (meters/second), angle of attack (degrees), frequency (Hertz), chord length (meters), and suction side displacement thickness (meters) \cite{airFoilDataset}. More information on the creation and properties of this dataset can be found in the following relevant papers \cite{nasaCiteRequest1} \cite{nasaCiteRequest2} \cite{nasaCiteRequest3}.

\subsection{Bike Sharing By Day (Bike Sharing)}
Hadi Fanaee from the Laboratory of Artificial Intelligence and Decision Support (LIAAD), University of Porto pulled together weather information and holiday information and integrated it with publically available trip data provided by Capital Bikeshare to generate this intriguing dataset consisting of two years of bike rental data aggregated both by day and by hour. Each instance contains attributes providing information about the day [and hour] that the rentals occurred. This includes the year, season, month, day, hour, whether it was a holiday, a weekday, or a working day. All of the time attributes are treated as categorical predictors in our experiment. In addition the weather conditions for each instant in time are provided including the precipitation type, temperature, humidity, and wind speed. Finally the count of bikes rented during each time period is provided, which will be used as the target variable in our experiment \cite{bikeSharingDataset} \cite{bikeSharingCiteRequest}. 

For our purposes we will look only at the daily data which limits the dataset to 731 instances rather than 17381 instances in the hourly version. No normalization or data cleaning has been applied to this dataset. This dataset is of course naturally quite noisy since people do not follow any hard and fast rules when deciding whether or not to go on a bike ride so the rental counts can vary considerably even for identical values of all the predictors.

\subsection{Communities and Crime (Crime Communities)}
Lastly we will look at the Communities and Crime dataset which contains 122 predictors for the ratio of violent crime to population in 1994 communities throughout the United States, where violent crimes are defined as murder, rape, robbery, and assault. Each example is comprised of data from the 1990 US Census, law enforcement data from the 1990 US LEMAS survey, and crime 
data from the 1995 FBI UCR. Some examples of the predictors provided are the household size, percent unemployed, divorce rates, ratio of police offices to population, number of different kinds of drugs seized by the police, average rent costs, percentage of variance races and ethnicities, and many more aspects of the community and local police force. Note that in this dataset all of the variables have been normalized to the range \([\ 0, 1\ ]\) and missing values are prevalent for many of the predictors. \cite{crimeCommunitiesDataset} \cite{crimeCommunitiesCiteRequest1} \cite{crimeCommunitiesCiteRequest2} \cite{crimeCommunitiesCiteRequest3} \cite{crimeCommunitiesCiteRequest4} \cite{crimeCommunitiesCiteRequest5}.

\section {Experiment}
In order to compare model performance of the original algorithm with our proposed shrinkage adaptation scheme, we developed an experiment in which a myriad of different parameter combinations were tested on the four datasets described in Section \ref{sec:datasets}. 

Gradient boosting machines with regression tree base learners are traditionally defined by the number of trees to be built and 4 additional parameters; the bag fraction, shrinkage, minimum number of examples in each leaf, and the maximum number of splits in each tree. To support our variable shrinkage scheme we must add one additional parameter since we require both a minimum and a maximum learning rate. All values that were considered for each of these parameters are provided in Table \ref{tab:parameters}. Taking all possible combinations of these parameters leads to 512 parameter sets with constant shrinkage, and 1536 possibilities with variable shrinkage for a total of 2048 individual tests for each dataset. 

All parameters that affect the execution of Algorithm \ref{alg:OptNumOfTrees}, which is the core training procedure used in all of our tests, are shown in Table \ref{tab:findOptTreeParam} for quick reference. 

In Table \ref{tab:executionLimits} we have the operating system, processor model and amount of memory of the systems used to run all of our tests. In addition Table  \ref{tab:executionLimits} lists the execution limits imposed on each individual test. Specifically, the running time and memory usage for during training was limited to 1.5 hours and 20 GB respectively. These values were exceeded only a handful of times when using the lowest constant learning rate and the highest number of splits. In the rare case that a test exceeded these limits the training stopped and the application continued with collecting and saving metadata about the constructed models, just as though the training stopped due to the normal stopping condition.

For a single run on a given dataset, all 2048 parameters were evaluated using the same 80\% training set / 20\% test set split. Selection of the test set examples was performed uniformly at random. The exact procedure used to evaluate a single set of parameters is provided in Algorithm \ref{alg:tuningProcedure}.

We performed two full runs for each dataset and computed the averages of data collected for each set of parameters. The results found in section \ref{sec:results} and discussed in \ref{sec:discussion} are derived from this averaged run data.

%---------------------------------------------------------------
\begin{table}
	\centering
		\begin{tabular}	{ | l | c |}
			\hline
			Bag Fraction & 0.25, 0.50, 0.75, 1 \\ \hline
			Min Examples In Node & 1, 10, 75, 150 \\ \hline
			Max Number Of Splits & 1, 2, 4, 8, 16, 32, 64, 128 \\ \hline
			Constant: Shrinkage & 0.1, 0.01, 0.001, 0.0001 \\ \hline
			Variable: Minimum Shrinkage & 0.01, 0.001, 0.0001 \\ \hline
			Variable: Maximum Shrinkage & 0.1, 0.4, 0.7, 1 \\
			\hline
			
		\end{tabular}
		\caption{All Tested GBM Model Parameters}
		\label{tab:parameters}
\end{table}
\begin{table}
	\centering
	\begin{tabular}{ | l | c |}
		\hline
		\rule{0pt}{2ex}	Maximum Number of Trees & 150,000 \\ \hline
		\rule{0pt}{2ex}	Number of CV Folds & 5 \\ \hline
		\rule{0pt}{2ex}	CV Step Size & 500 \\ \hline
		\rule{0pt}{2ex}	Max CV Steps w/o improving CV Error	& 3 \\ \hline
	\end{tabular}
	\caption{Parameters for Finding Optimal Number of Trees}
	\label{tab:findOptTreeParam}
\end{table}
\begin{table}
	\centering
	\begin{tabular}{ | l | c |}
		\hline
		\rule{0pt}{2ex} Operating System & Ubuntu 14.04.3 LTS \\ \hline
		\rule{0pt}{2ex} CPU & Intel i5-3570 \\ \hline
		\rule{0pt}{2ex} System Memory & 32 GB \\ \hline
		\rule{0pt}{2ex} Maximum Running Time & 1.5 hours \\ \hline
		\rule{0pt}{2ex} Maximum Memory Usage & 20 GB \\ \hline
	\end{tabular}
	\caption{CPU and Memory Hardware and Limits}
	\label{tab:executionLimits}
\end{table}
%---------------------------------------------------------------

\begin{algorithm}[]
	\hspace{1 mm} \\\hrule\hspace{1 mm} \\
	\begin{enumerate}
		\item Use Algorithm \ref{alg:OptNumOfTrees} to find the optimal number of trees. 
		\item Simultaneously train a GBM using the entire training set (All Training Data GBM), stop training when Algorithm \ref{alg:OptNumOfTrees}'s stopping condition is reached.
		\item Build an Aggregated Boosted Tree from the K GBMs built for cross validation.
		\item Collect and save meta-data about the run. This includes the running time, optimal number of trees, and the root mean squared error values at the optimal number of trees for...
		\begin{enumerate}
			\item The cross validation error (CV RMSE)
			\item The generalization error of the All Training Data GBM (ATD RMSE)
			\item The generalization error of the Aggregated Boosted Tree (ABT RMSE)
		\end{enumerate}
	\end{enumerate}
	\caption{Procedure for a Single Test.}
	\hspace{1 mm} \\\hrule\hspace{1 mm} \\\hspace{1 mm} \\
	\label{alg:tuningProcedure}
\end{algorithm}%

\begin{figure}[!t]

\end{figure}
%---------------------------------------------------------------
\section{Results}
\label{sec:results}

In Tables \ref{tab:powerPlantbestParameters}, \ref{tab:nasabestParameters}, \ref{tab:bikeSharingDaybestParameters}, and \ref{tab:crimeCommunitiesbestParameters} we provide the best constant and variable shrinkage parameters for the Power Plant, NASA, Bike Sharing, and Crime Communities datasets respectively. Each table consists of three subtables. The top subtable summarizes the parameters that achieved the best cross validation error, in the middle you will find the parameters that lead to the All Training Data GBM with minimal generalization error, and finally the bottom subtable presents the parameters that optimize the generalization error of their resulting Aggregated Boosted Tree. Note in an ideal world these would be equivalent, however we can never perfectly estimate generalization error.

For each set of parameters found in these tables, five measured attributes are provided that will be used throughout our discussion to compare performance achieved with constant vs. variable shrinkages. These will be referred to as "comparison attributes" and consist of the running time, the optimal number of trees, cross validation error, and generalization error of both the All Training Data GBMs and Aggregated Boosted Trees. Note all values for these attributes are averages taken across the two runs performed. The final column of each table provides the percent decrease between the performance of the best constant and best variable parameters. Since we'd like to minimize all of these quantities, a positive percent decrease indicates that the best variable shrinkage parameters performed better than their constant shrinkage counterparts, positive percent decrease are displayed in bold to quickly observe the cases in which variable shrinkage outperformed constant shrinkage in the best case. 

In order to uncover the overall impact of our proposed shrinkage adaptation scheme, we have generated plots to help visualize how each of the model parameters affects each comparison attribute when used with both constant and variable shrinkage, these will be referred to as "pairwise parameter influence plots" throughout the remainder of our discussion. The legend for the pairwise parameter influence plots is provided in Figure \ref{fig:legends} and the plots themselves are found in Figures
\ref{fig:parametersVsTimeInSeconds},
\ref{fig:parametersVsOptimalNumberOfTrees}, and
\ref{fig:parametersVsCvValidationError}.

Each pairwise parameter influence plot shows the relationship between one of the GBM model parameters (constant shrinkage, min shrinkage, max shrinkage, bag fraction, minimum leaf size, or max number of splits) and one of 3 comparison attributes (running time, optimal number of trees, or cross validation error). The plots for the generalization error of the All Training Data GBMs and Aggregated Boosted Trees have been left out since they are almost exactly the same as those for the cross validation error. This is to be expected of course since we use cross validation to estimate generalization error. 

We will however provide plots that show the relationship between cross validation error and the two forms of generalization error to see if the variable shrinkages have an impact on the generalization error estimation quality. These are found in Figure \ref{fig:cvVsGeneralization}.

To build these plots we first found the average comparison attributes across all datasets and all runs for all sets of parameters, resulting in a single set of 2048 records. Since we are comparing across datasets, 0-1 normalization is first performed on the comparison attributes within each dataset, then the normalized versions are averaged to create the final records used to generate in the plots. As a result, all of the pairwise parameter influence plots have y values ranging from 0 to 1 rather than the original ranges of the plotted attribute.


For each parameter and comparison attribute pair, two plots are generated. In the left columns of Figures
\ref{fig:parametersVsTimeInSeconds},
\ref{fig:parametersVsOptimalNumberOfTrees}, and
\ref{fig:parametersVsCvValidationError} you'll find scatter plots with one point for each of the 2048 sets of parameters tested, where the red points indicate tests using constant shrinkage and the blue points correspond to tests using variable shrinkage. To the right of each scatter plot you'll find a line plot where each (x, y) point is the average of all the x values at that particular y value in the scatter plot. Together they provide quite an informative view of how our variable shrinkage scheme affects gradient boosted regression trees overall, and will provide the basis of much of our discussion in section \ref{sec:discussion}.
	

\import{Z:/GBMWithVariableShrinkage/parameterTuning/5/}{entireResultsSection}

\newpage
%---------------------------------------------------------------
\section{Discussion}
\label{sec:discussion}

Overall, the results show that our initial hypothesis was well placed, and there are clear advantages to adapting the shrinkage applied to a leaf node's prediction based on the number of training examples that fell into that node. 

From the data shown in Tables \ref{tab:powerPlantbestParameters}, \ref{tab:nasabestParameters}, \ref{tab:bikeSharingDaybestParameters}, and \ref{tab:crimeCommunitiesbestParameters}, we can safely say that using variable learning rates does not impede the algorithm's ability to learn and generalize effectively, and in some cases actually results in lower RMSE than using constant shrinkage. However, its important to note that the differences between the error values for the best constant and variable parameters on all datasets are not large enough (only within a 6\% difference in all cases) to be considered very significant. So although the variable learning rates show lower minimum errors in all cases for the NASA as well as the Crime Communities dataset, but usually higher minimum errors for the Power Plant and Bike Sharing datasets; the more important result is that the minimum errors achieved with variable shrinkage are comparable in magnitude to those found with constant shrinkage, and not by any means significantly worse.

The really exciting result is the decrease in the amount of time and number of trees it usually takes to reach minimal error. For the Power Plant dataset we see an approximate 85\% reduction in running time across the board; and a corresponding reduction in the number of trees required for both the minimum CV Error and ADT Error. In the case of the Aggregated Boosted Trees, the best generalization was achieved using about the same number of trees with both constant and variable shrinkage, but each tree used only 4 splits rather than 64 with constant shrinkage, so here we are still looking at an overall decrease in modeled interactions of about 8 times. We see similar speedups and reduction in modeled interactions across the other three datasets, with only a couple exceptions. Namely the minimal cross validation error on the NASA and Bike Sharing datasets; yet on theses datasets we still see between a 35\% and 80\% decrease in the running time needed to achieve the minimal ATD and ABT generalization error. For the Crime Communities dataset, the running time to achieve the minimum error values is reduced by more than 20\% across the board.

Interestingly, Figure \ref{fig:cvVsGeneralization}, which plots the relationship between cross validation error and generalization errors clearly shows that on average the worst case parameters using variable shrinkage result in nearly half the error of the worst case constant shrinkage parameters. This is an exciting result which indicates that variable shrinkage leads to far better model stability in general and is extremely robust to changes in the model parameters. Such a result has obvious real world significance in that model builders could spend less time parameter tuning and still have confidence that their model will perform reasonably well. Looking at the individual parameters in turn, this overall robustness seems to have intuitive justification.

First consider the case where the GBM is given only a small number of splits it can work with. As always, the regression tree splitting algorithm will make the absolute most out of those splits, choosing at each step the split that results in the largest decrease in squared error (or largest increase in some other measure of node purity). Simple observation seems to indicate that the very first splits in regression trees are often quite unbalanced in nature, leading to large variance in the leaf sizes; and as the number of splits increases this variance diminishes. These initial, uneven, purity maximizing splits contain a lot of valuable information regarding example noise, outliers, and general generalization value that is ignored by simply applying the same small shrinkage to all nodes. With variable shrinkage the GBM is able to leverage this otherwise lost information to make the most out of every split, leading to faster error convergence on average. As the number of splits increases, and the size of the leave nodes become smaller and more evenly spread, it follows logically that the performance seen using constant and variable shrinkage should converge. This is precisely the relationship verified by the Max Number of Splits vs. Cross Validation Error plots in the first row of Figure \ref{fig:parametersVsCvValidationError}, and also explains how the models using variable shrinkage consistently require less running time and less trees to achieve error convergence as seen in the first rows of Figures \ref{fig:parametersVsTimeInSeconds} and \ref{fig:parametersVsOptimalNumberOfTrees}.

We can take a similar view of the minimum leaf size parameter. First observe from the second row of Figure \ref{fig:parametersVsTimeInSeconds}  that as the minimum leaf size increases, the average running time slowly descends at about the same rate for both constant and variable shrinkages. This is as expected due to the strong correlation between the actual number of splits made and the minimum leaf size. Larger minimum leaf sizes result in less possible splits which naturally will trim the running time of the algorithm. From Figure \ref{fig:parametersVsTimeInSeconds} we additionally see that the tests using variable shrinkage consistently take about half the time of their constant shrinkage counterparts. Again this is likely due to the fact that variable shrinkages allow the model to make use of more information contained in each split which leads to faster reduction of the overall error and a more timely meeting of the stopping condition. The correlation between leaf size and the actual number of splits becomes more apparent when looking at its relationship to the optimal number of trees and error shown in the second rows of Figures  \ref{fig:parametersVsOptimalNumberOfTrees} and \ref{fig:parametersVsCvValidationError}. As the leaf size increases, the difference in the number of trees required to converge becomes more pronounced. This phenomena likely occurs because the computed shrinkages will increase as the minimum node size increases; and larger shrinkage values implies faster learning and earlier stopping condition satisfaction. Although one might expect the use of these larger learning rates to result in a steep error ascent due to increased risk of over fitting, Figure \ref{fig:parametersVsCvValidationError} shows that the variable scheme keeps the model stable. Although the error rises as the leaf sizes increase, it rises with a gradient nearly identical to that of the constant tests, indicating once again that the implemented shrinkage weighting scheme enables the GBM to effectively utilize larger learning rates safely without increasing the risk of over fitting.
 
The bag fraction is a bit more tricky to analyze due to its inherent stochastic nature. When looking at the graphs for each individual dataset there seemed to be quite a bit of variance in which parameter values led to the fastest running time, lowest optimal number of trees, and optimal error; however these variations do appear to have averaged out to some observable trends shown in the third rows of Figures \ref{fig:parametersVsTimeInSeconds},
\ref{fig:parametersVsOptimalNumberOfTrees}, and
\ref{fig:parametersVsCvValidationError}. Firstly its once again apparent from Figure \ref{fig:parametersVsTimeInSeconds} that variable shrinkage leads to faster achievement of the stopping condition, less trees, and lower error on average. The running time curve expectedly shows that as the bag fraction increases, the running time increases on average; which makes sense due to the fact that more examples must be evaluated for splitting in the regression trees. The optimal number of trees curve shows an interesting trend in that the constant and variable shrinkage tests seem to be converging on the number of trees required as the bag fraction increases. This may be due to a decrease in the actual number of splits possible and potentially an increased likelihood of uneven splits when the bag fraction is low and less data is available to each regression tree. Once again the variable shrinkage scheme allows the GBM to make more effective use of all the split information so it requires less trees overall. As the bag fraction increases, the regression trees can make more splits leading to more even sized leaf nodes and an increased resemblance between the computed variable learning rates and the constant learning rates. Finally, in Figure \ref{fig:parametersVsCvValidationError} we see a slight decrease in average error as the bag fraction increases, yet from the scatter plot we see that the actual minimum errors are achieved using bag fractions less that one. This also follows closely with the intuition that the more training examples you have, the better your predictions will be in general, but is also supportive of Friedman's original reasoning in \cite{2002Friedman} that providing randomly selected subsets of the training data to each base learner decreases overall model variance, and thus error, by reducing correlation between the predictions made by the base learners built in each successive iteration.

The pairwise parameter influence plots for constant shrinkage are found in the fourth rows of Figures \ref{fig:parametersVsTimeInSeconds},
\ref{fig:parametersVsOptimalNumberOfTrees}, and
\ref{fig:parametersVsCvValidationError}. These plots confirm the trends that have been widely analyzed by previous Gradient Boosting Machine research. Increasing the shrinkage leads to less trees and faster stopping condition satisfaction, but at the cost of over fitting and increasing error. In many cases the tests using the two lowest constant shrinkage values of 0.0001 and 0.001 were unable to converge by the time they reached the maximum number of trees or ran out of time or memory, thus the error is actually the highest for these lowest error rates, but surely would have decreased if allowed to run to completion. Also note that the plots do not capture the error increase that will result from too high of constant shrinkage since the highest tested learning rate was 0.1. In early tests larger learning rates were evaluated and invariably resulted in unstable models and quick error divergence.

We will conclude our discussion with a look at the pairwise parameter influence plots for the min and max shrinkage parameters, which uncover what is perhaps the most surprising result of this study. It seems regardless of the values chosen for these parameters, the distribution of error values across all tests remains unchanged. Smaller values of these parameters do result in more trees and slightly longer running times on average, which makes sense since the average computed learning rates would be smaller and thus the model would learn more slowly. But the error value distribution is simply unaffected. This is great news as it in many ways nullifies the only major downside of implementing and using this scheme, the addition of another parameter to tune. Based on our results, it seems that in practice the min and max shrinkages could simply be set to any reasonable values, then parameter tuning would only need to be done on the remaining parameters. 

Our analysis strongly supports our initial hypothesis that by varying the shrinkage applied to the prediction of the examples in each individual leaf of regression tree base learners, the convergence time can be decreased without sacrificing generalization ability and resilience to over fitting. We have shown that the proposed variable shrinkage scheme consistently achieves comparable root mean squared error to constant shrinkage on a variety of real world datasets. In addition the pairwise parameter influence plots indicate that by dynamically computing shrinkage based on leaf node size, the algorithm as a whole becomes significantly more robust to variations in all model parameters. 

%---------------------------------------------------------------
\section{Conclusion}

A variable shrinkage scheme for gradient boosting machines with regression tree base learners was developed and tested against four real world datasets of various sizes, predictor dimensionality, and regression task difficulty. Under the proposed adaptation scheme, a separate shrinkage is calculated for each leaf node in each regression tree such that leaf nodes with less training examples receive lower shrinkage than larger sized leaf nodes. In this way less weight is given to the predictions of noisy, outlying, and atypical examples while the model focuses its learning to the responses of the more typical training examples. 

Our analysis shows that this adaptation scheme leads to a significant decrease in the total number of interactions that must be modeled in order to reach a minimum cross validation error. This decrease in running time and number of trees built is not accompanied by a decreased resilience to over fitting as one might expect, and has been shown to be capable of generating models with competitive and in some cases better root mean squared error vs. the best error achieved using a traditional constant shrinkage scheme.  

In addition, GBMs built with variable shrinkage have been shown to be far more robust to variations in all of the model parameters, leading to far lower error with the worst case variable shrinkage parameters than with the worst case constant shrinkage parameters. This property along with the decreased average running time makes our variable shrinkage scheme an attractive candidate for further research and use in production gradient boosting machine implementations.


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

\bibliographystyle{ieeetran}
	\bibliography{references}

% that's all folks
\end{document}


